import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
import random
import matplotlib.pyplot as plt
import time

# ============================================================================
# DEFINICJE KLAS
# ============================================================================

class DQN(nn.Module):
    """Sieƒá neuronowa Deep Q-Network"""
    
    def __init__(self, input_size, output_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, output_size)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

class ReplayBuffer:
    """Bufor do≈õwiadcze≈Ñ do przechowywania przej≈õƒá"""
    
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)
    
    def __len__(self):
        return len(self.buffer)

class DQNAgent:
    """Agent DQN do uczenia CartPole"""
    
    def __init__(self, env, learning_rate=0.001, gamma=0.99, epsilon=1.0, 
                 epsilon_min=0.01, epsilon_decay=0.995):
        self.env = env
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        
        # Inicjalizacja sieci neuronowych
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.policy_net = DQN(env.observation_space.shape[0], env.action_space.n).to(self.device)
        self.target_net = DQN(env.observation_space.shape[0], env.action_space.n).to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        
        # Optymalizator i pamiƒôƒá
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)
        self.memory = ReplayBuffer(10000)
        self.batch_size = 64
    
    def choose_action(self, state):
        """Wyb√≥r akcji z epsilon-greedy"""
        if random.random() < self.epsilon:
            return self.env.action_space.sample()
        
        with torch.no_grad():
            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            q_values = self.policy_net(state)
            return q_values.argmax().item()
    
    def train(self, num_episodes):
        """G≈Ç√≥wna pƒôtla treningu z zapisywaniem najlepszego modelu"""
        rewards_history = []
        best_avg_reward = 0
        best_episode = 0
        best_model_state = None

        for episode in range(num_episodes):
            # Reset ≈õrodowiska
            state, info = self.env.reset()
            total_reward = 0
            done = False
            
            # G≈Ç√≥wna pƒôtla epizodu
            while not done:
                action = self.choose_action(state)
                next_state, reward, terminated, truncated, info = self.env.step(action)
                done = terminated or truncated
                
                # Zapisz do≈õwiadczenie w buforze
                self.memory.push(state, action, reward, next_state, done)
                state = next_state
                total_reward += reward
                
                # Ucz sieƒá je≈õli mamy wystarczajƒÖco danych
                if len(self.memory) >= self.batch_size:
                    self._update_network()
            
            # Aktualizuj parametry
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
            
            # Aktualizuj sieƒá docelowƒÖ co 10 epizod√≥w
            if episode % 10 == 0:
                self.target_net.load_state_dict(self.policy_net.state_dict())
            
            rewards_history.append(total_reward)
            
            # Sprawd≈∫ postƒôp co 10 epizod√≥w
            if (episode + 1) % 10 == 0:
                avg_reward = np.mean(rewards_history[-10:])
                print(f"Epizod {episode + 1}, ≈örednia nagroda (ostatnie 10): {avg_reward:.2f}, Epsilon: {self.epsilon:.2f}")
                
                # Zapisz najlepszy model
                if avg_reward > best_avg_reward:
                    best_avg_reward = avg_reward
                    best_episode = episode + 1
                    best_model_state = self.policy_net.state_dict().copy()
                    print(f"üèÜ NOWY REKORD! Najlepsza ≈õrednia: {best_avg_reward:.2f} (epizod {best_episode})")
                    torch.save(best_model_state, 'best_cartpole_model.pth')
        
        # Za≈Çaduj najlepszy model na koniec treningu
        if best_model_state is not None:
            self.policy_net.load_state_dict(best_model_state)
            print(f"\nüìä PODSUMOWANIE TRENINGU:")
            print(f"Najlepsza ≈õrednia w ca≈Çym treningu: {best_avg_reward:.2f} (epizod {best_episode})")
            print(f"Ostatnia ≈õrednia (ostatnie 10 epizod√≥w): {np.mean(rewards_history[-10:]):.2f}")
            print(f"‚úÖ Za≈Çadowano najlepszy model do pamiƒôci")
        
        return rewards_history, best_avg_reward
    
    def _update_network(self):
        """Aktualizacja sieci neuronowej"""
        if len(self.memory) < self.batch_size:
            return
        
        # Pobierz pr√≥bkƒô z pamiƒôci
        batch = self.memory.sample(self.batch_size)
        state_batch = torch.FloatTensor([x[0] for x in batch]).to(self.device)
        action_batch = torch.LongTensor([[x[1]] for x in batch]).to(self.device)
        reward_batch = torch.FloatTensor([[x[2]] for x in batch]).to(self.device)
        next_state_batch = torch.FloatTensor([x[3] for x in batch]).to(self.device)
        done_batch = torch.FloatTensor([[x[4]] for x in batch]).to(self.device)
        
        # Oblicz warto≈õci Q
        current_q_values = self.policy_net(state_batch).gather(1, action_batch)
        next_q_values = self.target_net(next_state_batch).max(1)[0].detach().unsqueeze(1)
        expected_q_values = reward_batch + (1 - done_batch) * self.gamma * next_q_values
        
        # Oblicz stratƒô i zaktualizuj sieƒá
        loss = F.smooth_l1_loss(current_q_values, expected_q_values)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
    
    def play_episode(self, render=True, show_details=False):
        """Zagraj jeden epizod z aktualnƒÖ politykƒÖ"""
        state, info = self.env.reset()
        total_reward = 0
        done = False
        steps = 0
        
        while not done:
            if render:
                self.env.render()
                time.sleep(0.05)
            
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                q_values = self.policy_net(state_tensor)
                action = q_values.argmax().item()
            
            if show_details and steps % 10 == 0:
                print(f"Krok {steps}:")
                print(f"  Stan: [pozycja={state[0]:.3f}, prƒôdko≈õƒá={state[1]:.3f}, kƒÖt={state[2]:.3f}, prƒôdko≈õƒá_kƒÖtowa={state[3]:.3f}]")
                print(f"  Q-warto≈õci: {q_values.cpu().numpy()[0]}")
                print(f"  Wybrana akcja: {'LEWO' if action == 0 else 'PRAWO'}")
            
            state, reward, terminated, truncated, info = self.env.step(action)
            total_reward += reward
            done = terminated or truncated
            steps += 1
        
        return total_reward, steps

# ============================================================================
# G≈Å√ìWNY PROGRAM
# ============================================================================

def main():
    # Inicjalizacja ≈õrodowiska
    env = gym.make("CartPole-v1")
    print(f"≈örodowisko: {env.spec.id}")
    print(f"Przestrze≈Ñ obserwacji: {env.observation_space}")
    print(f"Przestrze≈Ñ akcji: {env.action_space}")

    # Trening agenta
    agent = DQNAgent(env)
    print("\nRozpoczynam trening DQN...")
    start_time = time.time()
    rewards_history, best_avg_reward = agent.train(num_episodes=1000)
    end_time = time.time()

    print(f"\nTrening zako≈Ñczony w {end_time - start_time:.2f} sekund")
    print(f"Najlepsza ≈õrednia nagroda w ca≈Çym treningu: {best_avg_reward:.2f}")

    env.close()

    # Wykresy postƒôpu
    plot_training_progress(rewards_history)
    
    # Demonstracja najlepszego rozwiƒÖzania
    demonstrate_best_solution(agent, best_avg_reward)

def plot_training_progress(rewards_history):
    """Wy≈õwietl wykresy postƒôpu treningu"""
    plt.figure(figsize=(12, 5))
    
    # Historia nagr√≥d
    plt.subplot(1, 2, 1)
    plt.plot(rewards_history)
    plt.title("Historia nagr√≥d podczas treningu")
    plt.xlabel("Epizod")
    plt.ylabel("Suma nagr√≥d")
    plt.grid(True)

    # ≈örednia kroczƒÖca
    window_size = 10
    moving_avg = [np.mean(rewards_history[max(0, i-window_size):i+1]) for i in range(len(rewards_history))]
    plt.subplot(1, 2, 2)
    plt.plot(moving_avg)
    plt.title(f"≈örednia kroczƒÖca (okno {window_size})")
    plt.xlabel("Epizod")
    plt.ylabel("≈örednia suma nagr√≥d")
    plt.grid(True)

    plt.tight_layout()
    plt.show()

def demonstrate_best_solution(agent, best_avg_reward):
    """Demonstracja najlepszego rozwiƒÖzania"""
    print("\n" + "="*60)
    print("WIZUALNA DEMONSTRACJA NAJLEPSZEGO ROZWIƒÑZANIA DQN")
    print("="*60)

    # Test bez wizualizacji
    test_performance_without_rendering(agent, best_avg_reward)
    
    # Wizualna demonstracja
    visual_demonstration(agent)

def test_performance_without_rendering(agent, best_avg_reward):
    """Test wydajno≈õci bez wizualizacji"""
    print("Testowanie modelu bez wizualizacji...")
    test_env = gym.make("CartPole-v1")
    agent.env = test_env
    agent.epsilon = 0  # Wy≈ÇƒÖcz eksploracjƒô

    pre_test_rewards = []
    for i in range(10):
        state, _ = test_env.reset()
        total_reward = 0
        done = False
        
        while not done:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)
                q_values = agent.policy_net(state_tensor)
                action = q_values.argmax().item()
            
            state, reward, terminated, truncated, _ = test_env.step(action)
            total_reward += reward
            done = terminated or truncated
        
        pre_test_rewards.append(total_reward)

    test_env.close()

    avg_test_reward = np.mean(pre_test_rewards)
    print(f"≈örednia nagroda w te≈õcie: {avg_test_reward:.1f}")
    print(f"Najlepszy wynik: {max(pre_test_rewards)}")
    print(f"Najgorszy wynik: {min(pre_test_rewards)}")

    # Fallback do zapisanego modelu je≈õli potrzeba
    if avg_test_reward < 400 and best_avg_reward >= 400:
        try:
            agent.policy_net.load_state_dict(torch.load('best_cartpole_model.pth'))
            print("üîÑ Za≈Çadowano zapisany najlepszy model")
        except:
            print("‚ö†Ô∏è Nie mo≈ºna za≈Çadowaƒá zapisanego modelu")

def visual_demonstration(agent):
    """Wizualna demonstracja z renderowaniem"""
    visual_env = gym.make("CartPole-v1", render_mode="human")
    agent.env = visual_env
    agent.epsilon = 0  # Wy≈ÇƒÖcz eksploracjƒô

    print("\nNaci≈õnij Enter, aby rozpoczƒÖƒá wizualnƒÖ demonstracjƒô...")
    input()

    test_rewards = []
    test_steps = []

    for episode in range(5):
        print(f"\n=== EPIZOD {episode + 1} ===")
        
        state, _ = visual_env.reset()
        total_reward = 0
        done = False
        steps = 0
        
        print(f"Stan poczƒÖtkowy: [pos={state[0]:.3f}, vel={state[1]:.3f}, angle={state[2]:.3f}, ang_vel={state[3]:.3f}]")
        
        while not done:
            visual_env.render()
            time.sleep(0.03)
            
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)
                q_values = agent.policy_net(state_tensor)
                action = q_values.argmax().item()
            
            # Szczeg√≥≈Çy dla pierwszego epizodu
            if episode == 0 and steps % 50 == 0:
                print(f"Krok {steps}: Q-warto≈õci=[{q_values.cpu().numpy()[0][0]:.3f}, {q_values.cpu().numpy()[0][1]:.3f}], "
                      f"Akcja={'LEWO' if action == 0 else 'PRAWO'}")
            
            state, reward, terminated, truncated, _ = visual_env.step(action)
            total_reward += reward
            done = terminated or truncated
            steps += 1
            
            if steps > 1000:  # Limit krok√≥w
                print("Epizod przerwany po 1000 krokach")
                break
        
        test_rewards.append(total_reward)
        test_steps.append(steps)
        
        # Ocena wyniku
        if total_reward >= 500:
            print(f"üéâ SUKCES! Epizod trwa≈Ç {steps} krok√≥w")
        elif total_reward >= 200:
            print(f"üëç DOBRY WYNIK! {steps} krok√≥w")
        else:
            print(f"‚ö†Ô∏è S≈Çup upad≈Ç po {steps} krokach")
        
        print(f"Suma nagr√≥d: {total_reward}")
        
        if episode < 4:
            print("\nNaci≈õnij Enter dla nastƒôpnego epizodu...")
            input()

    visual_env.close()
    
    # Podsumowanie demonstracji
    print_demonstration_summary(test_rewards, test_steps)

def print_demonstration_summary(test_rewards, test_steps):
    """Wydrukuj podsumowanie demonstracji"""
    print(f"\n=== PODSUMOWANIE DEMONSTRACJI ===")
    
    # Wyniki ka≈ºdego epizodu
    for i, (reward, steps) in enumerate(zip(test_rewards, test_steps)):
        status = "üéâ SUKCES" if reward >= 500 else "üëç DOBRY" if reward >= 200 else "‚ö†Ô∏è S≈ÅABY"
        print(f"  Epizod {i+1}: {steps:3d} krok√≥w, {reward:3.0f} nagr√≥d {status}")

    # Statystyki og√≥lne
    print(f"\nStatystyki:")
    print(f"≈örednia liczba krok√≥w: {np.mean(test_steps):.1f}")
    print(f"≈örednia suma nagr√≥d: {np.mean(test_rewards):.1f}")
    print(f"Najd≈Çu≈ºszy epizod: {max(test_steps)} krok√≥w")
    print(f"Najkr√≥tszy epizod: {min(test_steps)} krok√≥w")

    # Analiza sukcesu
    successful_episodes = sum(1 for r in test_rewards if r >= 500)
    good_episodes = sum(1 for r in test_rewards if r >= 200)

    print(f"\nWyniki:")
    print(f"Epizody sukcesu (‚â•500 krok√≥w): {successful_episodes}/5 ({successful_episodes/5*100:.0f}%)")
    print(f"Epizody dobre (‚â•200 krok√≥w): {good_episodes}/5 ({good_episodes/5*100:.0f}%)")

    # Ostateczna ocena
    if successful_episodes >= 4:
        print("üèÜ DOSKONA≈ÅY WYNIK! Model w pe≈Çni opanowa≈Ç zadanie!")
    elif successful_episodes >= 2:
        print("üéØ ≈öWIETNY WYNIK! Model bardzo dobrze balansuje!")
    elif good_episodes >= 3:
        print("üëç DOBRY WYNIK! Model pokazuje solidne umiejƒôtno≈õci!")
    else:
        print("üîÑ Model potrzebuje wiƒôcej treningu...")

if __name__ == "__main__":
    main()