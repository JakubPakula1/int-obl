import gymnasium as gym
import numpy as np
import pygad
import time

# UtwÃ³rz Å›rodowisko FrozenLake 8x8 bez poÅ›lizgÃ³w
env = gym.make("FrozenLake8x8-v1", is_slippery=False)
print(f"Åšrodowisko: {env.spec.id}")
print(f"PrzestrzeÅ„ obserwacji: {env.observation_space}")
print(f"PrzestrzeÅ„ akcji: {env.action_space}")

# Parametry chromosomu
CHROMOSOME_LENGTH = 30  # Maksymalna dÅ‚ugoÅ›Ä‡ sekwencji ruchÃ³w
POPULATION_SIZE = 200 # WiÄ™ksza populacja dla lepszej eksploracji
NUM_GENERATIONS = 300    # WiÄ™cej generacji dla zÅ‚oÅ¼onego problemu

def fitness_function(ga_instance, solution, solution_idx):
    """
    Funkcja fitness dla algorytmu genetycznego w FrozenLake8x8.
    
    CHROMOSOM: Lista liczb caÅ‚kowitych [0,1,2,3] reprezentujÄ…cych ruchy:
    - 0: LEFT (lewo)
    - 1: DOWN (dÃ³Å‚) 
    - 2: RIGHT (prawo)
    - 3: UP (gÃ³ra)
    
    FUNKCJA FITNESS:
    1. Symuluje grÄ™ wykonujÄ…c ruchy z chromosomu
    2. Ocenia pozycjÄ™ koÅ„cowÄ… agenta na mapie 8x8
    3. Przyznaje punkty za:
       - Dotarcie do celu (G) = +100 punktÃ³w + bonus za szybkoÅ›Ä‡
       - OdlegÅ‚oÅ›Ä‡ od celu = maksymalne punkty za bycie blisko
       - Kara za wpadniÄ™cie w dziurÄ™ (H) = -50 punktÃ³w
    4. UÅ¼ywa odlegÅ‚oÅ›ci Manhattan do obliczenia bliskoÅ›ci celu
    """
    observation, info = env.reset(seed=42)  # StaÅ‚y seed dla reprodukowalnoÅ›ci
    total_reward = 0
    steps_taken = 0
    max_steps = len(solution)
    
    # Pozycja celu w FrozenLake 8x8 (prawy dolny rÃ³g)
    goal_position = (7, 7)  # Wiersz 7, kolumna 7
    
    for action in solution:
        prev_observation = observation
        observation, reward, terminated, truncated, info = env.step(action)
        steps_taken += 1
        
        # JeÅ›li dotarliÅ›my do celu
        if terminated and reward > 0:
            # DuÅ¼a nagroda za sukces
            total_reward += 100
            # Bonus za szybkie dotarcie (mniej krokÃ³w = wiÄ™cej punktÃ³w)
            speed_bonus = (max_steps - steps_taken) / max_steps * 50
            total_reward += speed_bonus
            print(f"Sukces! DotarÅ‚ do celu w {steps_taken} krokach")
            break
        
        # JeÅ›li wpadliÅ›my w dziurÄ™
        if terminated and reward == 0:
            total_reward -= 50  # Kara za wpadniÄ™cie w dziurÄ™
            break
        
        # Nagroda za postÄ™p w kierunku celu (na podstawie pozycji)
        # Konwertuj numer pola na wspÃ³Å‚rzÄ™dne (x, y)
        current_pos = (observation // 8, observation % 8)
        
        # Oblicz odlegÅ‚oÅ›Ä‡ Manhattan od celu
        distance_to_goal = abs(current_pos[0] - goal_position[0]) + abs(current_pos[1] - goal_position[1])
        
        # Im bliÅ¼ej celu, tym wiÄ™cej punktÃ³w (maksymalnie 14 punktÃ³w za bycie w celu)
        proximity_reward = (14 - distance_to_goal) * 2
        total_reward += proximity_reward
        
        # Dodatkowa nagroda za ruch w dobrym kierunku
        prev_pos = (prev_observation // 8, prev_observation % 8)
        prev_distance = abs(prev_pos[0] - goal_position[0]) + abs(prev_pos[1] - goal_position[1])
        
        if distance_to_goal < prev_distance:
            total_reward += 5  # Bonus za zbliÅ¼enie siÄ™ do celu
        elif distance_to_goal > prev_distance:
            total_reward -= 2  # MaÅ‚a kara za oddalenie siÄ™
    
    return max(0, total_reward)  # Fitness nie moÅ¼e byÄ‡ ujemny

def on_generation(ga_instance):
    """Callback wywoÅ‚ywany po kaÅ¼dej generacji - monitoring postÄ™pu"""
    best_fitness = ga_instance.best_solution()[1]
    avg_fitness = np.mean(ga_instance.last_generation_fitness)
    print(f"Generacja {ga_instance.generations_completed}: Najlepszy={best_fitness:.2f}, Åšredni={avg_fitness:.2f}")

# Konfiguracja algorytmu genetycznego
ga_instance = pygad.GA(
    num_generations=NUM_GENERATIONS,
    num_parents_mating=20,  # WiÄ™cej rodzicÃ³w dla wiÄ™kszej rÃ³Å¼norodnoÅ›ci
    fitness_func=fitness_function,
    sol_per_pop=POPULATION_SIZE,
    num_genes=CHROMOSOME_LENGTH,
    gene_type=int,
    gene_space=[0, 1, 2, 3],  # MoÅ¼liwe ruchy
    mutation_type="random",
    mutation_percent_genes=15,  # WyÅ¼sza mutacja dla eksploracji
    crossover_type="single_point",
    parent_selection_type="tournament",
    on_generation=on_generation,
    random_seed=42  # Dla reprodukowalnoÅ›ci wynikÃ³w
)

# Uruchom algorytm genetyczny
print("Rozpoczynam trening algorytmu genetycznego dla FrozenLake 8x8...")
start_time = time.time()
ga_instance.run()
end_time = time.time()

# PokaÅ¼ najlepsze rozwiÄ…zanie
best_solution, best_fitness, _ = ga_instance.best_solution()
print(f"\nTrening zakoÅ„czony w {end_time - start_time:.2f} sekund")
print(f"Najlepsze rozwiÄ…zanie:")
print(f"WartoÅ›Ä‡ funkcji fitness: {best_fitness:.2f}")

env.close()
# Wizualna demonstracja najlepszego rozwiÄ…zania z render_mode="human"
print("\n" + "="*50)
print("WIZUALNA DEMONSTRACJA NAJLEPSZEGO ROZWIÄ„ZANIA")
print("="*50)
move_names = {0: "LEFT", 1: "DOWN", 2: "RIGHT", 3: "UP"}
# UtwÃ³rz nowe Å›rodowisko z trybem wizualnym
visual_env = gym.make("FrozenLake8x8-v1", is_slippery=False, render_mode="human")

observation, info = visual_env.reset(seed=42)
total_reward = 0
step_count = 0

print("NaciÅ›nij Enter, aby rozpoczÄ…Ä‡ wizualnÄ… demonstracjÄ™...")
input()

for action in best_solution:
    print(f"\nKrok {step_count + 1}: {move_names[action]}")
    print(f"Obecna pozycja: {observation}")
    
    observation, reward, terminated, truncated, info = visual_env.step(action)
    total_reward += reward
    step_count += 1
    
    # Pauza miÄ™dzy ruchami dla lepszej obserwacji
    time.sleep(1)
    
    if terminated or truncated:
        if reward > 0:
            print(f"\nğŸ‰ SUKCES! Agent dotarÅ‚ do celu w {step_count} krokach!")
        else:
            print(f"\nâŒ PORAÅ»KA! Agent wpadÅ‚ w dziurÄ™ po {step_count} krokach.")
        break

print(f"\nKoÅ„cowa suma nagrÃ³d: {total_reward}")
print("NaciÅ›nij Enter, aby zamknÄ…Ä‡ okno gry...")
input()
visual_env.close()

# PokaÅ¼ statystyki treningu
# PokaÅ¼ statystyki treningu
ga_instance.plot_fitness()