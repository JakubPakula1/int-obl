import gymnasium as gym
import numpy as np
import time
import matplotlib.pyplot as plt

class Particle:
    def __init__(self, num_actions, num_steps):
        # Pozycja czƒÖstki to sekwencja ruch√≥w
        self.position = np.random.randint(0, 2, num_steps)  # 0 lub 1 (lewo/prawo)
        self.velocity = np.random.uniform(-0.5, 0.5, num_steps)  # Losowa prƒôdko≈õƒá poczƒÖtkowa
        self.best_position = self.position.copy()
        self.best_score = float('-inf')

class PSO:
    def __init__(self, num_particles, num_steps, env, w=0.7, c1=1.5, c2=1.5):
        self.num_particles = num_particles
        self.num_steps = num_steps
        self.env = env
        self.w = w  # Wsp√≥≈Çczynnik bezw≈Çadno≈õci
        self.c1 = c1  # Wsp√≥≈Çczynnik uczenia osobistego
        self.c2 = c2  # Wsp√≥≈Çczynnik uczenia spo≈Çecznego
        
        # Inicjalizacja roju
        self.particles = [Particle(2, num_steps) for _ in range(num_particles)]
        self.global_best_position = None
        self.global_best_score = float('-inf')
        
        # Historia dla wykres√≥w
        self.best_scores_history = []
        self.avg_scores_history = []
    
    def evaluate_particle(self, particle):
        """
        Ocena czƒÖstki poprzez symulacjƒô w ≈õrodowisku.
        
        REPREZENTACJA CZƒÑSTKI:
        - Position: Tablica k-wymiarowa (k=num_steps)
        - Ka≈ºdy wymiar reprezentuje jednƒÖ akcjƒô w sekwencji
        - Warto≈õci 0/1 dla dyskretnych akcji (lewo/prawo)
        
        FUNKCJA FITNESS:
        - Podstawowa nagroda za ka≈ºdy krok = +1
        - Bonus za d≈Çugie epizody (>100 krok√≥w)
        - Kara za wczesne zako≈Ñczenie
        """
        observation, info = self.env.reset(seed=42)  # Sta≈Çy seed dla reprodukowalno≈õci
        total_reward = 0
        steps_taken = 0
        
        for action in particle.position:
            observation, reward, terminated, truncated, info = self.env.step(action)
            total_reward += reward
            steps_taken += 1
            
            if terminated or truncated:
                break
        
        # Dodatkowe nagrody za wydajno≈õƒá
        if steps_taken >= 195:  # Maksymalny wynik w CartPole
            total_reward += 50  # Bonus za perfekcyjny wynik
        elif steps_taken > 100:
            total_reward += (steps_taken - 100) * 0.1  # Bonus za d≈Çugie utrzymanie
        
        return total_reward
    
    def update_velocity(self, particle):
        """
        Aktualizacja prƒôdko≈õci wed≈Çug wzoru PSO.
        W przestrzeni dyskretnej u≈ºywamy probabilistycznego podej≈õcia.
        """
        if self.global_best_position is None:
            return
            
        r1, r2 = np.random.rand(2)
        
        # Obliczanie nowej prƒôdko≈õci
        cognitive = self.c1 * r1 * (particle.best_position - particle.position)
        social = self.c2 * r2 * (self.global_best_position - particle.position)
        particle.velocity = (self.w * particle.velocity + cognitive + social)
        
        # Ograniczenie prƒôdko≈õci dla stabilno≈õci
        particle.velocity = np.clip(particle.velocity, -2, 2)
    
    def update_position(self, particle):
        """
        Aktualizacja pozycji z adaptacjƒÖ do przestrzeni dyskretnej.
        U≈ºywamy funkcji sigmoidalnej do konwersji prƒôdko≈õci na prawdopodobie≈Ñstwa.
        """
        # Aktualizacja pozycji z prƒôdko≈õciƒÖ
        continuous_position = particle.position + particle.velocity
        
        # Konwersja do przestrzeni dyskretnej za pomocƒÖ sigmoid
        probabilities = 1 / (1 + np.exp(-continuous_position))
        
        # Pr√≥bkowanie binarne na podstawie prawdopodobie≈Ñstw
        particle.position = (np.random.rand(self.num_steps) < probabilities).astype(int)
    
    def optimize(self, num_iterations, verbose=True):
        """G≈Ç√≥wna pƒôtla optymalizacji PSO z monitoringiem postƒôpu"""
        for iteration in range(num_iterations):
            scores = []
            
            for particle in self.particles:
                # Ocena czƒÖstki
                score = self.evaluate_particle(particle)
                scores.append(score)
                
                # Aktualizacja najlepszej pozycji czƒÖstki
                if score > particle.best_score:
                    particle.best_score = score
                    particle.best_position = particle.position.copy()
                
                # Aktualizacja globalnej najlepszej pozycji
                if score > self.global_best_score:
                    self.global_best_score = score
                    self.global_best_position = particle.position.copy()
                
                # Aktualizacja prƒôdko≈õci i pozycji
                self.update_velocity(particle)
                self.update_position(particle)
            
            # Zapisz statystyki
            self.best_scores_history.append(self.global_best_score)
            self.avg_scores_history.append(np.mean(scores))
            
            # Adaptacyjne zmniejszanie bezw≈Çadno≈õci
            self.w = max(0.1, self.w * 0.99)
            
            if verbose and (iteration + 1) % 10 == 0:  # Wy≈õwietlaj co 10 iteracji
                print(f"Iteracja {iteration + 1}: Najlepszy={self.global_best_score:.1f}, "
                      f"≈öredni={np.mean(scores):.1f}, w={self.w:.3f}")

def train_pso_fast():
    """Szybki trening PSO bez wizualizacji"""
    print("=== FAZA TRENINGU PSO ===")
    
    # ≈örodowisko bez renderowania dla szybkiego treningu
    env_train = gym.make("CartPole-v1")
    print(f"≈örodowisko treningu: {env_train.spec.id}")
    print(f"Przestrze≈Ñ obserwacji: {env_train.observation_space}")
    print(f"Przestrze≈Ñ akcji: {env_train.action_space}")

    # Parametry PSO
    NUM_PARTICLES = 30
    NUM_STEPS = 200  # Maksymalna d≈Çugo≈õƒá sekwencji
    NUM_ITERATIONS = 100

    # Utw√≥rz i uruchom PSO
    print(f"\nRozpoczynam szybkƒÖ optymalizacjƒô PSO...")
    print(f"Parametry: {NUM_PARTICLES} czƒÖstek, {NUM_STEPS} krok√≥w, {NUM_ITERATIONS} iteracji")
    
    start_time = time.time()
    pso = PSO(NUM_PARTICLES, NUM_STEPS, env_train)
    pso.optimize(NUM_ITERATIONS, verbose=True)
    end_time = time.time()

    print(f"\n‚úÖ Optymalizacja zako≈Ñczona w {end_time - start_time:.2f} sekund")
    print(f"üèÜ Najlepszy wynik: {pso.global_best_score:.1f}")
    
    env_train.close()
    return pso

def show_training_results(pso):
    """Wy≈õwietl wykresy postƒôpu treningu"""
    print("\n=== ANALIZA WYNIK√ìW TRENINGU ===")
    
    plt.figure(figsize=(12, 8))
    
    # Wykres 1: Postƒôp fitness
    plt.subplot(2, 2, 1)
    plt.plot(pso.best_scores_history, label='Najlepszy wynik', linewidth=2, color='green')
    plt.plot(pso.avg_scores_history, label='≈öredni wynik', alpha=0.7, color='blue')
    plt.xlabel('Iteracja')
    plt.ylabel('Fitness')
    plt.title('Postƒôp PSO w CartPole')
    plt.legend()
    plt.grid(True)
    
    # Wykres 2: Ostatnie 20 iteracji (zoom)
    plt.subplot(2, 2, 2)
    last_20_best = pso.best_scores_history[-20:]
    last_20_avg = pso.avg_scores_history[-20:]
    plt.plot(range(len(pso.best_scores_history)-20, len(pso.best_scores_history)), 
             last_20_best, label='Najlepszy (ostatnie 20)', linewidth=2, color='red')
    plt.plot(range(len(pso.avg_scores_history)-20, len(pso.avg_scores_history)), 
             last_20_avg, label='≈öredni (ostatnie 20)', alpha=0.7, color='orange')
    plt.xlabel('Iteracja')
    plt.ylabel('Fitness')
    plt.title('Ostatnie 20 iteracji')
    plt.legend()
    plt.grid(True)
    
    # Wykres 3: Histogram najlepszych wynik√≥w
    plt.subplot(2, 2, 3)
    plt.hist(pso.best_scores_history, bins=20, alpha=0.7, color='purple')
    plt.xlabel('Fitness')
    plt.ylabel('Czƒôsto≈õƒá')
    plt.title('Rozk≈Çad najlepszych wynik√≥w')
    plt.grid(True)
    
    # Wykres 4: Statystyki
    plt.subplot(2, 2, 4)
    stats_text = f"""
Statystyki treningu:
‚Ä¢ Najlepszy wynik: {max(pso.best_scores_history):.1f}
‚Ä¢ ≈öredni wynik ko≈Ñcowy: {pso.avg_scores_history[-1]:.1f}
‚Ä¢ Poprawa: {pso.best_scores_history[-1] - pso.best_scores_history[0]:.1f}
‚Ä¢ Stabilno≈õƒá (std): {np.std(pso.best_scores_history[-10:]):.2f}
"""
    plt.text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center')
    plt.axis('off')
    plt.title('Statystyki')
    
    plt.tight_layout()
    plt.show()

def demonstrate_solution(pso):
    """Demonstracja najlepszego rozwiƒÖzania z wizualizacjƒÖ"""
    print("\n=== DEMONSTRACJA NAJLEPSZEGO ROZWIƒÑZANIA ===")
    
    # ≈örodowisko z renderowaniem tylko do demonstracji
    env_demo = gym.make("CartPole-v1", render_mode="human")
    
    print("üéÆ Uruchamiam demonstracjƒô najlepszego rozwiƒÖzania...")
    print("Naci≈õnij Enter aby rozpoczƒÖƒá demonstracjƒô...")
    input()
    
    observation, info = env_demo.reset(seed=42)
    total_reward = 0
    step_count = 0
    
    action_names = {0: "‚¨ÖÔ∏è LEWO", 1: "‚û°Ô∏è PRAWO"}
    
    print(f"Rozpoczynam symulacjƒô z {len(pso.global_best_position)} zaplanowanymi ruchami...")
    
    for i, action in enumerate(pso.global_best_position):
        observation, reward, terminated, truncated, info = env_demo.step(action)
        total_reward += reward
        step_count += 1
        
        # Wy≈õwietl informacje co kilka krok√≥w
        if step_count % 20 == 0 or step_count <= 10:
            print(f"Krok {step_count:3d}: {action_names[action]} | "
                  f"Pozycja: {observation[0]:6.3f} | "
                  f"KƒÖt: {observation[2]:6.3f} | "
                  f"Nagroda: {total_reward:.0f}")
        
        time.sleep(0.05)  # Pauza dla wizualizacji
        
        if terminated or truncated:
            break
    
    print(f"\nüèÅ Epizod zako≈Ñczony!")
    print(f"üìä Statystyki:")
    print(f"   ‚Ä¢ Czas trwania: {step_count} krok√≥w")
    print(f"   ‚Ä¢ Ko≈Ñcowa suma nagr√≥d: {total_reward}")
    print(f"   ‚Ä¢ Skuteczno≈õƒá: {(step_count/200)*100:.1f}% (max 200 krok√≥w)")
    
    # Ocena wynik√≥w
    if step_count >= 195:
        print("üèÜ DOSKONA≈ÅY WYNIK! Perfekcyjne rozwiƒÖzanie!")
    elif step_count >= 150:
        print("ü•á BARDZO DOBRY WYNIK!")
    elif step_count >= 100:
        print("ü•à DOBRY WYNIK!")
    else:
        print("ü•â WYNIK DO POPRAWY")
    
    env_demo.close()

def main():
    """G≈Ç√≥wna funkcja ≈ÇƒÖczƒÖca trening i demonstracjƒô"""
    print("üöÄ OPTYMALIZACJA CARTPOLE PRZY U≈ªYCIU PSO")
    print("=" * 50)
    
    # Faza 1: Szybki trening
    trained_pso = train_pso_fast()
    
    # Faza 2: Analiza wynik√≥w
    show_training_results(trained_pso)
    
    # Faza 3: Demonstracja
    demonstrate_solution(trained_pso)
    
    print("\n‚úÖ Program zako≈Ñczony pomy≈õlnie!")

# Uruchomienie programu
if __name__ == "__main__":
    main()