import argparse
import numpy as np
from environments.car_racing_env import CarRacingEnv
from agents.neat_agent import NEATAgent
from agents.dqn_agent import DQNAgent
from agents.random_agent import RandomAgent
from training.train_neat import train_neat, continue_from_checkpoint as continue_neat
from training.train_dqn import train_dqn, continue_from_checkpoint as continue_dqn
from evaluation.evaluate import evaluate
from training.train_ppo import train_ppo
import os
import time
import glob

def main():
    # Parsowanie argumentÃ³w wiersza poleceÅ„
    parser = argparse.ArgumentParser(description='Car Racing RL')
    parser.add_argument('--agent', type=str, default='neat', 
                       choices=['neat', 'dqn', 'random', 'ppo', 'fuzzy', 'genetic', 'pso'],
                       help='Rodzaj agenta do trenowania/testowania')
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'],
                        help='Tryb pracy: trenowanie lub testowanie')
    parser.add_argument('--model', type=str, default=None,
                        help='ÅšcieÅ¼ka do modelu do wczytania (dla trybu test)')
    parser.add_argument('--continue_training', action='store_true',
                        help='Kontynuuj trening z ostatniego checkpointu')
    parser.add_argument('--episodes', type=int, default=10,
                        help='Liczba epizodÃ³w treningu/testowania')
    parser.add_argument('--max_steps', type=int, default=200,
                        help='Maksymalna liczba krokÃ³w w epizodzie')
    parser.add_argument('--render', action='store_true',
                        help='WyÅ›wietlanie Å›rodowiska w trakcie treningu')
    
    args = parser.parse_args()
    
    # Tworzenie Å›rodowiska
    render_mode = "human" if args.render else None
    
    
    # WybÃ³r agenta i trybu
    if args.mode == 'train':
        if args.agent == 'neat':
            env = CarRacingEnv(render_mode=render_mode)
            train_neat_agent(env, args)
        elif args.agent == 'dqn':
            env = CarRacingEnv(render_mode=render_mode, continuous=False)
            train_dqn_agent(env, args)
        elif args.agent == 'ppo':
            env = CarRacingEnv(render_mode=render_mode, continuous=True)  # PPO uÅ¼ywa ciÄ…gÅ‚ych akcji
            train_ppo_agent(env, args)
        elif args.agent == 'genetic':
            env = CarRacingEnv(render_mode=render_mode, continuous=True)
            train_genetic_agent(env, args)
        elif args.agent == 'pso':
            env = CarRacingEnv(render_mode=render_mode, continuous=True)
            train_pso_agent(env, args)
        elif args.agent == 'random':
            print("Agent losowy nie wymaga treningu")
    elif args.mode == 'test':
        if args.agent == 'neat':
            env = CarRacingEnv(render_mode=render_mode)
            test_neat_agent(env, args)
        elif args.agent == 'dqn':
            env = CarRacingEnv(render_mode=render_mode, continuous=False)
            test_dqn_agent(env, args)
        elif args.agent == 'ppo':
            env = CarRacingEnv(render_mode=render_mode, continuous=True)
            test_ppo_agent(env, args)
        elif args.agent == 'fuzzy':
            env = CarRacingEnv(render_mode=render_mode, continuous=True)
            test_fuzzy_agent(env, args)
        elif args.agent == 'genetic':
            env = CarRacingEnv(render_mode=render_mode, continuous=True)
            test_genetic_agent(env, args)
        elif args.agent == 'pso':
            env = CarRacingEnv(render_mode=render_mode, continuous=True)
            test_pso_agent(env, args)
        elif args.agent == 'random':
            env = CarRacingEnv(render_mode=render_mode, continuous=True)
            test_random_agent(env, args)
        
    
    env.close()

def train_neat_agent(env, args):
    if args.continue_training:
        # TYMCZASOWA POPRAWKA: UÅ¼yj konkretnego checkpointu
        latest_checkpoint = 'checkpoints/neat-checkpoint-10'  # â† ZMIEÅƒ NA NAJNOWSZY
        
        if os.path.exists(latest_checkpoint):
            print(f"Kontynuowanie z checkpointu: {latest_checkpoint}")
            neat_agent, winner = continue_neat(latest_checkpoint, env, args.episodes)
        else:
            print("Szukam najnowszego checkpointu...")
            checkpoints = glob.glob('checkpoints/neat-checkpoint-*')
            if checkpoints:
                def extract_number(filename):
                    import re
                    match = re.search(r'checkpoint-(\d+)', filename)
                    return int(match.group(1)) if match else 0
                
                latest_checkpoint = max(checkpoints, key=extract_number)
                print(f"Kontynuowanie z checkpointu: {latest_checkpoint}")
                neat_agent, winner = continue_neat(latest_checkpoint, env, args.episodes)
            else:
                print("Nie znaleziono checkpointÃ³w, rozpoczynanie nowego treningu...")
                neat_agent, winner = train_neat(env, generations=args.episodes)
    else:
        print("Rozpoczynanie nowego treningu NEAT...")
        neat_agent, winner = train_neat(env, generations=args.episodes)

def train_dqn_agent(env, args):
    from models.neural_networks import DQNNetwork
    
    if args.continue_training:
        checkpoints = glob.glob('checkpoints/dqn/dqn_model_ep*.keras')
        if checkpoints:
            # Sortowanie numeryczne zamiast alfabetycznego
            def extract_episode_number(filename):
                import re
                match = re.search(r'ep(\d+)', filename)
                return int(match.group(1)) if match else 0
            
            latest_checkpoint = max(checkpoints, key=extract_episode_number)
            print(f"Kontynuowanie z checkpointu: {latest_checkpoint}")
            agent = DQNAgent.load(latest_checkpoint, (84,84,1), 5)
        else:
            print("Nie znaleziono checkpointÃ³w, rozpoczynanie nowego treningu...")
            model = DQNNetwork(input_shape=(84, 84, 1), action_space=5).model
            agent = DQNAgent((84, 84, 1), 5, model)
    else:
        print("Rozpoczynanie nowego treningu DQN...")
        model = DQNNetwork(input_shape=(84, 84, 1), action_space=5).model
        agent = DQNAgent((84, 84, 1), 5, model)
    
    # Trenowanie agenta DQN
    train_dqn(env, agent, episodes=args.episodes)

def test_neat_agent(env, args):
    """Testowanie agenta NEAT - POPRAWIONA WERSJA"""
    from agents.neat_agent import NEATAgent
    
    if args.model:
        print(f"Wczytywanie modelu: {args.model}")
        # POPRAWKA: UÅ¼yj metody klasowej load_model
        neat_agent = NEATAgent.load_model(args.model, 'configs/neat_config.txt')
        if neat_agent is None:
            print("âŒ Nie udaÅ‚o siÄ™ wczytaÄ‡ modelu!")
            return
    else:
        # ZnajdÅº najnowszy model
        import glob
        models = glob.glob('models/neat_*.pkl')
        if models:
            latest_model = max(models, key=os.path.getctime)
            print(f"Wczytywanie najnowszego modelu: {latest_model}")
            neat_agent = NEATAgent.load_model(latest_model, 'configs/neat_config.txt')
        else:
            print("âŒ Nie znaleziono modeli NEAT!")
            return
    
    print(f"ğŸ§¬ Model NEAT wczytany - fitness: {neat_agent.best_genome.fitness:.2f}")
    
    # Testowanie
    total_rewards = []
    
    for episode in range(args.episodes):
        print(f"\n=== EPIZOD {episode+1}/{args.episodes} ===")
        observation, info = env.reset()  # POPRAWKA: PrawidÅ‚owy unpacking
        
        episode_reward = 0
        steps = 0
        start_time = time.time()
        
        for step in range(1000):
            action = neat_agent.act(observation)
            observation, reward, terminated, truncated, info = env.step(action)
            
            episode_reward += reward
            steps += 1
            
            if terminated or truncated:
                break
        
        total_rewards.append(episode_reward)
        episode_time = time.time() - start_time
        
        tiles_visited = info.get('tiles_visited', 0)
        print(f"Epizod {episode+1}: {episode_reward:.2f} pts, {steps} krokÃ³w, {tiles_visited} pÅ‚ytek, {episode_time:.1f}s")
    
    # Statystyki
    avg_reward = np.mean(total_rewards)
    print(f"\nğŸ¯ PODSUMOWANIE NEAT:")
    print(f"ğŸ“Š Åšrednia nagroda: {avg_reward:.2f}")
    print(f"ğŸ† Najlepszy wynik: {max(total_rewards):.2f}")
    print(f"ğŸ“‰ Najgorszy wynik: {min(total_rewards):.2f}")


def test_dqn_agent(env, args):
    """Dedykowana funkcja testowania DQN z preprocessingiem"""
    from training.train_dqn import preprocess_state
    import time
    from environments.lap_completion_fix_wrapper import LapCompletionFixWrapper
    env = LapCompletionFixWrapper(env)
    # Wczytaj model
    if args.model:
        agent = DQNAgent.load(args.model, (84, 84, 1), 5)
    else:
        models = glob.glob('checkpoints/dqn/*.keras')
        if models:
            def extract_episode_number(filename):
                import re
                match = re.search(r'ep(\d+)', filename)
                return int(match.group(1)) if match else 0
            
            latest_model = max(models, key=extract_episode_number)
            print(f"Wczytywanie modelu: {latest_model}")
            agent = DQNAgent.load(latest_model, (84, 84, 1), 5)
        else:
            print("Nie znaleziono modelu DQN")
            return
    
    # Epsilon = 0 dla czystego testowania (tylko eksploatacja)
    agent.epsilon = 0.0
    print(f"Model: epsilon={agent.epsilon}, epizody treningowe={agent.episodes}")
    
    # Testowanie
    total_rewards = []
    total_steps = []
    completed_laps = 0
    
    for episode in range(args.episodes):
        print(f"\n=== EPIZOD {episode+1}/{args.episodes} ===")
        observation,info = env.reset()
        observation = preprocess_state(observation)
        
        episode_reward = 0
        steps = 0
        start_time = time.time()
        
        for step in range(1000):
            action = agent.act(observation)
            next_observation, reward, terminated, truncated, info = env.step(action)
            next_observation = preprocess_state(next_observation)
            
            observation = next_observation
            episode_reward += reward
            steps += 1
            
            # WyÅ›wietl progress co 100 krokÃ³w
            if steps % 100 == 0:
                print(f"  Krok {steps}, Nagroda: {episode_reward:.2f}")
                print(info)
            
            if terminated or truncated:
                break
        
        episode_time = time.time() - start_time
        total_rewards.append(episode_reward)
        total_steps.append(steps)
        
        # Ocena rezultatu epizodu
        if terminated and episode_reward > 600:
            completed_laps += 1
            result_text = "ğŸ† TOR UKOÅƒCZONY!"
        elif terminated and episode_reward > 300:
            result_text = "ğŸš— Dobra jazda - prawie ukoÅ„czyÅ‚!"
        elif terminated and episode_reward > 0:
            result_text = "âœ… Pozytywny wynik"
        elif terminated:
            result_text = "âŒ WypadÅ‚ z toru"
        else:
            result_text = "â±ï¸ Timeout - przekroczono limit czasu"
        
        print(f"Epizod {episode+1} ukoÅ„czony:")
        print(f"  Kroki: {steps}/1000")
        print(f"  Nagroda: {episode_reward:.2f}")
        print(f"  Czas: {episode_time:.2f}s")
        print(f"  Wynik: {result_text}")
        
        # Dodatkowe informacje o postÄ™pie
        if episode_reward > 800:
            progress = "95-100%"
        elif episode_reward > 600:
            progress = "80-95%"
        elif episode_reward > 400:
            progress = "60-80%"
        elif episode_reward > 200:
            progress = "30-60%"
        elif episode_reward > 0:
            progress = "0-30%"
        else:
            progress = "Problemy z jazdÄ…"
        
        print(f"  Szacowany postÄ™p na torze: ~{progress}")
    
    # Podsumowanie koÅ„cowe
    print(f"\n{'='*50}")
    print(f"=== PODSUMOWANIE TESTÃ“W DQN ===")
    print(f"{'='*50}")
    print(f"Åšrednia nagroda: {np.mean(total_rewards):.2f} Â± {np.std(total_rewards):.2f}")
    print(f"Najlepszy wynik: {max(total_rewards):.2f}")
    print(f"Najgorszy wynik: {min(total_rewards):.2f}")
    print(f"Åšrednia liczba krokÃ³w: {np.mean(total_steps):.1f}")
    print(f"NajdÅ‚uÅ¼szy epizod: {max(total_steps)} krokÃ³w")
    print(f"NajkrÃ³tszy epizod: {min(total_steps)} krokÃ³w")
    
    # Analiza sukcesu
    print(f"\nAnaliza wynikÃ³w:")
    successful_runs = sum(1 for r in total_rewards if r > 600)
    good_runs = sum(1 for r in total_rewards if r > 300)
    positive_runs = sum(1 for r in total_rewards if r > 0)
    
    print(f"UkoÅ„czone tory (>600 pkt): {successful_runs}/{args.episodes} ({successful_runs/args.episodes*100:.1f}%)")
    print(f"Dobre wyniki (>300 pkt): {good_runs}/{args.episodes} ({good_runs/args.episodes*100:.1f}%)")
    print(f"Pozytywne wyniki (>0 pkt): {positive_runs}/{args.episodes} ({positive_runs/args.episodes*100:.1f}%)")
    
    # Ocena ogÃ³lna modelu
    print(f"\nOcena modelu:")
    avg_reward = np.mean(total_rewards)
    if successful_runs >= args.episodes * 0.8:
        print("ğŸ† DOSKONAÅY MODEL! Konsekwentnie ukaÅ„cza tory.")
    elif successful_runs >= args.episodes * 0.6:
        print("ğŸŒŸ BARDZO DOBRY MODEL! CzÄ™sto ukaÅ„cza tory.")
    elif successful_runs >= args.episodes * 0.4:
        print("ğŸ‘ DOBRY MODEL! Czasami ukaÅ„cza tory.")
    elif good_runs >= args.episodes * 0.6:
        print("âš¡ PRZYZWOITY MODEL! Dobrze jeÅºdzi, ale rzadko ukaÅ„cza.")
    elif positive_runs >= args.episodes * 0.6:
        print("ğŸ”„ SÅABY MODEL! Wymaga wiÄ™cej treningu.")
    else:
        print("âŒ BARDZO SÅABY MODEL! Potrzebuje znacznie wiÄ™cej treningu.")
    
    # Rekomendacje
    print(f"\nRekomendacje:")
    if successful_runs < args.episodes * 0.5:
        print("â€¢ Kontynuuj trening modelu")
        print("â€¢ RozwaÅ¼ dostrojenie hiperparametrÃ³w")
        print("â€¢ SprawdÅº funkcjÄ™ nagrody")
    else:
        print("â€¢ Model jest dobrze wytrenowany!")
        print("â€¢ MoÅ¼esz uÅ¼yÄ‡ go do dalszych eksperymentÃ³w")

def train_ppo_agent(env, args):
    """Trenowanie agenta PPO"""
    from agents.ppo_agent import PPOAgent
    from training.train_ppo import train_ppo
    
    if args.continue_training:
        # Implementacja kontynuacji treningu PPO
        import glob
        actor_models = glob.glob('checkpoints/ppo/*_actor.keras')
        if actor_models:
            def extract_episode_number(filename):
                import re
                match = re.search(r'ep(\d+)', filename)
                return int(match.group(1)) if match else 0
            
            latest_actor = max(actor_models, key=extract_episode_number)
            latest_critic = latest_actor.replace('_actor.keras', '_critic.keras')
            
            print(f"Kontynuowanie z modeli: {latest_actor}, {latest_critic}")
            agent = PPOAgent.load(latest_actor, latest_critic, (84, 84, 1), 3)
        else:
            print("Nie znaleziono modeli PPO, rozpoczynanie nowego treningu...")
            agent = PPOAgent((84, 84, 1), 3)
    else:
        print("Rozpoczynanie nowego treningu PPO...")
        agent = PPOAgent((84, 84, 1), 3)
    
    # train_ppo(env, agent, episodes=args.episodes)
    train_ppo(env, agent, episodes=args.episodes)

def test_ppo_agent(env, args):
    from stable_baselines3 import PPO
    import time
    from environments.lap_completion_fix_wrapper import LapCompletionFixWrapper
    import sys

    env = LapCompletionFixWrapper(env)
    model_path = "models/ppo_carracing1"

    if not os.path.exists(model_path + ".zip"):
        print(f"âŒ Model nie znaleziony: {model_path}.zip")
        print("DostÄ™pne modele:")
        if os.path.exists("models/"):
            for file in os.listdir("models/"):
                if file.endswith(('.zip', '.pkl')):
                    print(f"  - {file}")
        sys.exit(1)

    try:
        model = PPO.load(model_path)
        print(f"âœ… Model wczytany: {model_path}")
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d wczytywania modelu: {e}")
        sys.exit(1)

    episodes = 5
    total_rewards = []

    print(f"ğŸš€ Rozpoczynanie ewaluacji PPO na {episodes} epizodÃ³w")

    for ep in range(episodes):
        print(f"\n=== EPIZOD {ep + 1}/{episodes} ===")
        obs, info = env.reset()
        done = False
        total_reward = 0
        steps = 0

        while not done:
            action, _ = model.predict(obs, deterministic=True)  # deterministic=True dla testÃ³w
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            steps += 1
            done = terminated or truncated
            
            # Opcjonalne spowolnienie dla lepszej wizualizacji
            time.sleep(1/60)
            
            # Zabezpieczenie przed nieskoÅ„czonÄ… pÄ™tlÄ…
            if steps > 1000:
                print("â° Timeout - przerwano epizod")
                break

        total_rewards.append(total_reward)
        
        # Ocena wyniku
        if terminated and total_reward > 600:
            result = "ğŸ† TOR UKOÅƒCZONY!"
        elif terminated and total_reward > 300:
            result = "ğŸš— Dobra jazda!"
        elif total_reward > 0:
            result = "âœ… Pozytywny wynik"
        else:
            result = "âŒ SÅ‚aby wynik"
        
        print(f"Epizod {ep + 1}: {steps} krokÃ³w, {total_reward:.2f} pkt - {result}")
        
        # Informacje o postÄ™pie na torze
        tiles_visited = info.get('tiles_visited', 0)
        total_tiles = info.get('total_tiles', 0)
        if total_tiles > 0:
            progress = (tiles_visited / total_tiles) * 100
            print(f"PostÄ™p na torze: {tiles_visited}/{total_tiles} pÅ‚ytek ({progress:.1f}%)")

    # Podsumowanie
    print(f"\n{'='*50}")
    print(f"=== PODSUMOWANIE EWALUACJI PPO ===")
    print(f"{'='*50}")
    print(f"Åšrednia nagroda: {sum(total_rewards)/len(total_rewards):.2f}")
    print(f"Najlepszy wynik: {max(total_rewards):.2f}")
    print(f"Najgorszy wynik: {min(total_rewards):.2f}")

    # Analiza sukcesu
    successful_runs = sum(1 for r in total_rewards if r > 600)
    good_runs = sum(1 for r in total_rewards if r > 300)
    positive_runs = sum(1 for r in total_rewards if r > 0)

    print(f"\nAnaliza wynikÃ³w:")
    print(f"UkoÅ„czone tory (>600 pkt): {successful_runs}/{episodes} ({successful_runs/episodes*100:.1f}%)")
    print(f"Dobre wyniki (>300 pkt): {good_runs}/{episodes} ({good_runs/episodes*100:.1f}%)")
    print(f"Pozytywne wyniki (>0 pkt): {positive_runs}/{episodes} ({positive_runs/episodes*100:.1f}%)")

    env.close()
    print("\nâœ… Ewaluacja zakoÅ„czona!")
def train_genetic_agent(env, args):
    """Trenowanie agenta genetycznego"""
    from agents.ga_agent import GeneticAgent
    
    agent = GeneticAgent(env, chromosome_length=600, population_size=30)
    agent.train(num_generations=args.episodes)

def train_pso_agent(env, args):
    """Trenowanie agenta PSO"""
    from agents.pso_agent import PSOAgent
    
    agent = PSOAgent(env, num_particles=20, dimensions=600)
    agent.train(max_iterations=args.episodes)

def test_fuzzy_agent(env, args):
    """Testowanie agenta rozmytego"""
    from agents.fuzzy_agent import FuzzyAgent
    
    agent = FuzzyAgent()
    
    total_rewards = []
    for episode in range(args.episodes):
        print(f"\n=== EPIZOD {episode+1}/{args.episodes} ===")
        observation, info = env.reset()
        episode_reward = 0
        steps = 0
        
        for step in range(1000):
            action = agent.act(observation)
            observation, reward, terminated, truncated, info = env.step(action)
            episode_reward += reward
            steps += 1
            
            if terminated or truncated:
                break
        
        total_rewards.append(episode_reward)
        print(f"Epizod {episode+1}: {steps} krokÃ³w, {episode_reward:.2f} pkt")
    
    print(f"\nğŸ§  PODSUMOWANIE FUZZY:")
    print(f"ğŸ“Š Åšrednia nagroda: {np.mean(total_rewards):.2f}")

def test_genetic_agent(env, args):
    """Testowanie agenta genetycznego"""
    from agents.ga_agent import GeneticAgent
    
    if args.model:
        agent = GeneticAgent.load_best_genome(args.model, env)
    else:
        agent = GeneticAgent.load_best_genome('models/genetic_best_final.pkl', env)
    
    if agent is None:
        print("âŒ Nie moÅ¼na wczytaÄ‡ modelu genetycznego")
        return
    
    total_rewards = []
    for episode in range(args.episodes):
        print(f"\n=== EPIZOD {episode+1}/{args.episodes} ===")
        observation, info = env.reset()
        episode_reward = 0
        steps = 0
        
        for step in range(1000):
            action = agent.act(observation, step)
            observation, reward, terminated, truncated, info = env.step(action)
            episode_reward += reward
            steps += 1
            
            if terminated or truncated:
                break
        
        total_rewards.append(episode_reward)
        print(f"Epizod {episode+1}: {steps} krokÃ³w, {episode_reward:.2f} pkt")
    
    print(f"\nğŸ§¬ PODSUMOWANIE GENETIC:")
    print(f"ğŸ“Š Åšrednia nagroda: {np.mean(total_rewards):.2f}")

def test_pso_agent(env, args):
    """Testowanie agenta PSO"""
    from agents.pso_agent import PSOAgent
    
    if args.model:
        agent = PSOAgent.load_model(args.model, env)
    else:
        agent = PSOAgent.load_model('models/pso_best_final.pkl', env)
    
    if agent is None:
        print("âŒ Nie moÅ¼na wczytaÄ‡ modelu PSO")
        return
    
    total_rewards = []
    for episode in range(args.episodes):
        print(f"\n=== EPIZOD {episode+1}/{args.episodes} ===")
        observation, info = env.reset()
        episode_reward = 0
        steps = 0
        
        for step in range(1000):
            action = agent.act(observation, step)
            observation, reward, terminated, truncated, info = env.step(action)
            episode_reward += reward
            steps += 1
            
            if terminated or truncated:
                break
        
        total_rewards.append(episode_reward)
        print(f"Epizod {episode+1}: {steps} krokÃ³w, {episode_reward:.2f} pkt")
    
    print(f"\nğŸ PODSUMOWANIE PSO:")
    print(f"ğŸ“Š Åšrednia nagroda: {np.mean(total_rewards):.2f}")

def test_random_agent(env, args):
    """Testowanie agenta losowego - baseline"""
    from agents.random_agent import RandomAgent
    import time
    import numpy as np
    
    agent = RandomAgent()
    print(f"ğŸ² Testowanie agenta losowego - {args.episodes} epizodÃ³w")
    
    total_rewards = []
    total_steps = []
    completed_laps = 0
    
    for episode in range(args.episodes):
        print(f"\n=== EPIZOD {episode+1}/{args.episodes} ===")
        observation, info = env.reset()
        
        episode_reward = 0
        steps = 0
        start_time = time.time()
        
        for step in range(1000):
            # Agent losowy nie potrzebuje obserwacji - dziaÅ‚ania sÄ… losowe
            action = agent.act()
            observation, reward, terminated, truncated, info = env.step(action)
            
            episode_reward += reward
            steps += 1
            
            # WyÅ›wietl progress co 200 krokÃ³w
            if steps % 200 == 0:
                print(f"  Krok {steps}, Nagroda: {episode_reward:.2f}")
            
            if terminated or truncated:
                break
        
        episode_time = time.time() - start_time
        total_rewards.append(episode_reward)
        total_steps.append(steps)
        
        # Ocena rezultatu epizodu
        if terminated and episode_reward > 600:
            completed_laps += 1
            result_text = "ğŸ† TOR UKOÅƒCZONY! (niesamowite szczÄ™Å›cie!)"
        elif terminated and episode_reward > 300:
            result_text = "ğŸ€ Bardzo szczÄ™Å›liwy przejazd!"
        elif terminated and episode_reward > 100:
            result_text = "ğŸ² Przyzwoity losowy wynik"
        elif terminated and episode_reward > 0:
            result_text = "âœ… Pozytywny wynik"
        elif terminated:
            result_text = "âŒ WypadÅ‚ z toru (typowe dla losowego)"
        else:
            result_text = "â±ï¸ Timeout - przekroczono limit czasu"
        
        print(f"Epizod {episode+1} ukoÅ„czony:")
        print(f"  Kroki: {steps}/1000")
        print(f"  Nagroda: {episode_reward:.2f}")
        print(f"  Czas: {episode_time:.2f}s")
        print(f"  Wynik: {result_text}")
    
    # Podsumowanie koÅ„cowe
    print(f"\n{'='*50}")
    print(f"=== PODSUMOWANIE TESTÃ“W RANDOM ===")
    print(f"{'='*50}")
    
    try:
        mean_reward = sum(total_rewards) / len(total_rewards)
        std_reward = (sum((r - mean_reward)**2 for r in total_rewards) / len(total_rewards))**0.5
        mean_steps = sum(total_steps) / len(total_steps)
        
        print(f"Åšrednia nagroda: {mean_reward:.2f} Â± {std_reward:.2f}")
        print(f"Najlepszy wynik: {max(total_rewards):.2f}")
        print(f"Najgorszy wynik: {min(total_rewards):.2f}")
        print(f"Åšrednia liczba krokÃ³w: {mean_steps:.1f}")
        print(f"NajdÅ‚uÅ¼szy epizod: {max(total_steps)} krokÃ³w")
        print(f"NajkrÃ³tszy epizod: {min(total_steps)} krokÃ³w")
        
        # Analiza sukcesu
        print(f"\nAnaliza wynikÃ³w (baseline losowy):")
        successful_runs = sum(1 for r in total_rewards if r > 600)
        good_runs = sum(1 for r in total_rewards if r > 300)
        decent_runs = sum(1 for r in total_rewards if r > 100)
        positive_runs = sum(1 for r in total_rewards if r > 0)
        
        print(f"UkoÅ„czone tory (>600 pkt): {successful_runs}/{args.episodes} ({successful_runs/args.episodes*100:.1f}%)")
        print(f"Bardzo dobre (>300 pkt): {good_runs}/{args.episodes} ({good_runs/args.episodes*100:.1f}%)")
        print(f"Przyzwoite (>100 pkt): {decent_runs}/{args.episodes} ({decent_runs/args.episodes*100:.1f}%)")
        print(f"Pozytywne wyniki (>0 pkt): {positive_runs}/{args.episodes} ({positive_runs/args.episodes*100:.1f}%)")
        
        # Ocena jako baseline
        print(f"\nğŸ’¡ Analiza baseline:")
        if successful_runs >= 1:
            print("ğŸ¤¯ NIESPOTYKANE SZCZÄ˜ÅšCIE! Agent losowy ukoÅ„czyÅ‚ tor!")
        elif good_runs >= args.episodes * 0.1:
            print("ğŸ€ Niezwykle szczÄ™Å›liwy agent losowy!")
        elif decent_runs >= args.episodes * 0.3:
            print("ğŸ² Typowy agent losowy - czasami ma szczÄ™Å›cie")
        elif positive_runs >= args.episodes * 0.5:
            print("ğŸ“Š Normalny baseline - okoÅ‚o poÅ‚owy wynikÃ³w pozytywnych")
        else:
            print("âŒ Bardzo pechowy agent losowy")
        
        # Znaczenie jako baseline
        print(f"\nğŸ“ˆ Znaczenie jako baseline:")
        print(f"â€¢ KaÅ¼dy inteligentny agent powinien osiÄ…gaÄ‡ lepsze wyniki")
        print(f"â€¢ Åšrednia nagroda {mean_reward:.2f} to minimum do pokonania")
        print(f"â€¢ WskaÅºnik sukcesu {successful_runs/args.episodes*100:.1f}% to prÃ³g referencyjny")
        
        if mean_reward > 0:
            print(f"â€¢ âœ… Baseline wydaje siÄ™ rozsÄ…dny")
        else:
            print(f"â€¢ âš ï¸ Bardzo niski baseline - Å›rodowisko moÅ¼e byÄ‡ trudne")
        
        return {
            'avg_reward': mean_reward,
            'success_rate': successful_runs/args.episodes*100,
            'total_rewards': total_rewards
        }
    
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d w obliczeniach: {e}")
        return None
  # Dla innych agentÃ³w
if __name__ == "__main__":
    main()