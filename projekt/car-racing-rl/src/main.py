import argparse
import numpy as np
from environments.car_racing_env import CarRacingEnv
from agents.neat_agent import NEATAgent
from agents.dqn_agent import DQNAgent
from agents.random_agent import RandomAgent
from training.train_neat import train_neat, continue_from_checkpoint as continue_neat
from training.train_dqn import train_dqn, continue_from_checkpoint as continue_dqn
from evaluation.evaluate import evaluate
import os
import glob

def main():
    # Parsowanie argumentÃ³w wiersza poleceÅ„
    parser = argparse.ArgumentParser(description='Car Racing RL')
    parser.add_argument('--agent', type=str, default='neat', choices=['neat', 'dqn', 'random', 'ppo'],
                        help='Rodzaj agenta do trenowania/testowania')
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'],
                        help='Tryb pracy: trenowanie lub testowanie')
    parser.add_argument('--model', type=str, default=None,
                        help='ÅšcieÅ¼ka do modelu do wczytania (dla trybu test)')
    parser.add_argument('--continue_training', action='store_true',
                        help='Kontynuuj trening z ostatniego checkpointu')
    parser.add_argument('--episodes', type=int, default=10,
                        help='Liczba epizodÃ³w treningu/testowania')
    parser.add_argument('--max_steps', type=int, default=200,
                        help='Maksymalna liczba krokÃ³w w epizodzie')
    parser.add_argument('--render', action='store_true',
                        help='WyÅ›wietlanie Å›rodowiska w trakcie treningu')
    
    args = parser.parse_args()
    
    # Tworzenie Å›rodowiska
    render_mode = "human" if args.render else None
    
    
    # WybÃ³r agenta i trybu
    if args.mode == 'train':
        if args.agent == 'neat':
            env = CarRacingEnv(render_mode=render_mode)
            train_neat_agent(env, args)
        elif args.agent == 'dqn':
            env = CarRacingEnv(render_mode=render_mode, continuous=False)
            train_dqn_agent(env, args)
        elif args.agent == 'ppo':
            env = CarRacingEnv(render_mode=render_mode, continuous=True)  # PPO uÅ¼ywa ciÄ…gÅ‚ych akcji
            train_ppo_agent(env, args)
        elif args.agent == 'random':
            print("Agent losowy nie wymaga treningu")
    elif args.mode == 'test':
        if args.agent == 'neat':
            env = CarRacingEnv(render_mode=render_mode)
            test_agent(env, args)
        elif args.agent == 'dqn':
            env = CarRacingEnv(render_mode=render_mode, continuous=False)
            test_dqn_agent(env, args)
        elif args.agent == 'ppo':
            env = CarRacingEnv(render_mode=render_mode, continuous=True)
            test_ppo_agent(env, args)
        elif args.agent == 'random':
            print("Agent losowy nie wymaga treningu")
        
    
    env.close()

def train_neat_agent(env, args):
    if args.continue_training:
        checkpoints = glob.glob('checkpoints/neat-checkpoint-*')
        if checkpoints:
            latest_checkpoint = sorted(checkpoints)[-1]
            print(f"Kontynuowanie z checkpointu: {latest_checkpoint}")
            neat_agent, winner = continue_neat(latest_checkpoint, env, args.episodes)
        else:
            print("Nie znaleziono checkpointÃ³w, rozpoczynanie nowego treningu...")
            neat_agent, winner = train_neat(env, generations=args.episodes)
    else:
        print("Rozpoczynanie nowego treningu NEAT...")
        neat_agent, winner = train_neat(env, generations=args.episodes)

def train_dqn_agent(env, args):
    from models.neural_networks import DQNNetwork
    
    if args.continue_training:
        checkpoints = glob.glob('checkpoints/dqn/dqn_model_ep*.keras')
        if checkpoints:
            # Sortowanie numeryczne zamiast alfabetycznego
            def extract_episode_number(filename):
                import re
                match = re.search(r'ep(\d+)', filename)
                return int(match.group(1)) if match else 0
            
            latest_checkpoint = max(checkpoints, key=extract_episode_number)
            print(f"Kontynuowanie z checkpointu: {latest_checkpoint}")
            agent = DQNAgent.load(latest_checkpoint, (84,84,1), 5)
        else:
            print("Nie znaleziono checkpointÃ³w, rozpoczynanie nowego treningu...")
            model = DQNNetwork(input_shape=(84, 84, 1), action_space=5).model
            agent = DQNAgent((84, 84, 1), 5, model)
    else:
        print("Rozpoczynanie nowego treningu DQN...")
        model = DQNNetwork(input_shape=(84, 84, 1), action_space=5).model
        agent = DQNAgent((84, 84, 1), 5, model)
    
    # Trenowanie agenta DQN
    train_dqn(env, agent, episodes=args.episodes)

def test_agent(env, args):
    
    if args.agent == 'neat':
        env_visual = CarRacingEnv(render_mode="human")
        print(f"Action space: {env_visual.action_space}")
        print(f"Action space type: {type(env_visual.action_space)}")
        if args.model:
            # Wczytaj konkretny model
            import pickle
            with open(args.model, 'rb') as f:
                agent = pickle.load(f)
            neat_agent = NEATAgent(agent)
        else:
            # Wczytaj najnowszy model
            models = glob.glob('models/neat*.pkl')
            if models:
                latest_model = sorted(models)[-1]
                print(f"Wczytywanie modelu: {latest_model}")
                import pickle
                with open(latest_model, 'rb') as f:
                    agent = pickle.load(f)
                neat_agent = NEATAgent(agent)
            else:
                print("Nie znaleziono modelu NEAT")
                return
        agent = neat_agent
            
    elif args.agent == 'dqn':
        env_visual = CarRacingEnv(render_mode="human", continuous=False)
        print(f"Action space: {env_visual.action_space}")
        print(f"Action space type: {type(env_visual.action_space)}")
        if args.model:
            # Wczytaj konkretny model
            agent = DQNAgent.load(args.model, (84, 84, 1), 5)
        else:
            # Wczytaj najnowszy model
            models = glob.glob('checkpoints/dqn/*.keras')
            if models:
                latest_model = sorted(models)[-1]
                print(f"Wczytywanie modelu: {latest_model}")
                agent = DQNAgent.load(latest_model, (84, 84, 1), 5)
            else:
                print("Nie znaleziono modelu DQN")
                return
    
                
    elif args.agent == 'random':
        agent = RandomAgent(5)  # 5 akcji
    
    else:
        print(f"Testowanie agenta {args.agent} nie jest zaimplementowane")
        return
    
    # Testowanie agenta
    print(f"Testowanie agenta {args.agent}...")
    observation = env_visual.reset()
    total_reward = 0
    steps = 0
    
    for _ in range(args.episodes * 1000):  # Maksymalna liczba krokÃ³w
        action = agent.act(observation)
        observation, reward, terminated, truncated, info = env_visual.step(action)
        total_reward += reward
        steps += 1
        
        if terminated or truncated:
            print(f"Epizod zakoÅ„czony po {steps} krokach z nagrodÄ…: {total_reward:.2f}")
            observation = env_visual.reset()
            total_reward = 0
            steps = 0
            
    env_visual.close()


def test_dqn_agent(env, args):
    """Dedykowana funkcja testowania DQN z preprocessingiem"""
    from training.train_dqn import preprocess_state
    import time
    from environments.lap_completion_fix_wrapper import LapCompletionFixWrapper
    env = LapCompletionFixWrapper(env)
    # Wczytaj model
    if args.model:
        agent = DQNAgent.load(args.model, (84, 84, 1), 5)
    else:
        models = glob.glob('checkpoints/dqn/*.keras')
        if models:
            def extract_episode_number(filename):
                import re
                match = re.search(r'ep(\d+)', filename)
                return int(match.group(1)) if match else 0
            
            latest_model = max(models, key=extract_episode_number)
            print(f"Wczytywanie modelu: {latest_model}")
            agent = DQNAgent.load(latest_model, (84, 84, 1), 5)
        else:
            print("Nie znaleziono modelu DQN")
            return
    
    # Epsilon = 0 dla czystego testowania (tylko eksploatacja)
    agent.epsilon = 0.0
    print(f"Model: epsilon={agent.epsilon}, epizody treningowe={agent.episodes}")
    
    # Testowanie
    total_rewards = []
    total_steps = []
    completed_laps = 0
    
    for episode in range(args.episodes):
        print(f"\n=== EPIZOD {episode+1}/{args.episodes} ===")
        observation,info = env.reset()
        observation = preprocess_state(observation)
        
        episode_reward = 0
        steps = 0
        start_time = time.time()
        
        for step in range(1000):
            action = agent.act(observation)
            next_observation, reward, terminated, truncated, info = env.step(action)
            next_observation = preprocess_state(next_observation)
            
            observation = next_observation
            episode_reward += reward
            steps += 1
            
            # WyÅ›wietl progress co 100 krokÃ³w
            if steps % 100 == 0:
                print(f"  Krok {steps}, Nagroda: {episode_reward:.2f}")
                print(info)
            
            if terminated or truncated:
                break
        
        episode_time = time.time() - start_time
        total_rewards.append(episode_reward)
        total_steps.append(steps)
        
        # Ocena rezultatu epizodu
        if terminated and episode_reward > 600:
            completed_laps += 1
            result_text = "ğŸ† TOR UKOÅƒCZONY!"
        elif terminated and episode_reward > 300:
            result_text = "ğŸš— Dobra jazda - prawie ukoÅ„czyÅ‚!"
        elif terminated and episode_reward > 0:
            result_text = "âœ… Pozytywny wynik"
        elif terminated:
            result_text = "âŒ WypadÅ‚ z toru"
        else:
            result_text = "â±ï¸ Timeout - przekroczono limit czasu"
        
        print(f"Epizod {episode+1} ukoÅ„czony:")
        print(f"  Kroki: {steps}/1000")
        print(f"  Nagroda: {episode_reward:.2f}")
        print(f"  Czas: {episode_time:.2f}s")
        print(f"  Wynik: {result_text}")
        
        # Dodatkowe informacje o postÄ™pie
        if episode_reward > 800:
            progress = "95-100%"
        elif episode_reward > 600:
            progress = "80-95%"
        elif episode_reward > 400:
            progress = "60-80%"
        elif episode_reward > 200:
            progress = "30-60%"
        elif episode_reward > 0:
            progress = "0-30%"
        else:
            progress = "Problemy z jazdÄ…"
        
        print(f"  Szacowany postÄ™p na torze: ~{progress}")
    
    # Podsumowanie koÅ„cowe
    print(f"\n{'='*50}")
    print(f"=== PODSUMOWANIE TESTÃ“W DQN ===")
    print(f"{'='*50}")
    print(f"Åšrednia nagroda: {np.mean(total_rewards):.2f} Â± {np.std(total_rewards):.2f}")
    print(f"Najlepszy wynik: {max(total_rewards):.2f}")
    print(f"Najgorszy wynik: {min(total_rewards):.2f}")
    print(f"Åšrednia liczba krokÃ³w: {np.mean(total_steps):.1f}")
    print(f"NajdÅ‚uÅ¼szy epizod: {max(total_steps)} krokÃ³w")
    print(f"NajkrÃ³tszy epizod: {min(total_steps)} krokÃ³w")
    
    # Analiza sukcesu
    print(f"\nAnaliza wynikÃ³w:")
    successful_runs = sum(1 for r in total_rewards if r > 600)
    good_runs = sum(1 for r in total_rewards if r > 300)
    positive_runs = sum(1 for r in total_rewards if r > 0)
    
    print(f"UkoÅ„czone tory (>600 pkt): {successful_runs}/{args.episodes} ({successful_runs/args.episodes*100:.1f}%)")
    print(f"Dobre wyniki (>300 pkt): {good_runs}/{args.episodes} ({good_runs/args.episodes*100:.1f}%)")
    print(f"Pozytywne wyniki (>0 pkt): {positive_runs}/{args.episodes} ({positive_runs/args.episodes*100:.1f}%)")
    
    # Ocena ogÃ³lna modelu
    print(f"\nOcena modelu:")
    avg_reward = np.mean(total_rewards)
    if successful_runs >= args.episodes * 0.8:
        print("ğŸ† DOSKONAÅY MODEL! Konsekwentnie ukaÅ„cza tory.")
    elif successful_runs >= args.episodes * 0.6:
        print("ğŸŒŸ BARDZO DOBRY MODEL! CzÄ™sto ukaÅ„cza tory.")
    elif successful_runs >= args.episodes * 0.4:
        print("ğŸ‘ DOBRY MODEL! Czasami ukaÅ„cza tory.")
    elif good_runs >= args.episodes * 0.6:
        print("âš¡ PRZYZWOITY MODEL! Dobrze jeÅºdzi, ale rzadko ukaÅ„cza.")
    elif positive_runs >= args.episodes * 0.6:
        print("ğŸ”„ SÅABY MODEL! Wymaga wiÄ™cej treningu.")
    else:
        print("âŒ BARDZO SÅABY MODEL! Potrzebuje znacznie wiÄ™cej treningu.")
    
    # Rekomendacje
    print(f"\nRekomendacje:")
    if successful_runs < args.episodes * 0.5:
        print("â€¢ Kontynuuj trening modelu")
        print("â€¢ RozwaÅ¼ dostrojenie hiperparametrÃ³w")
        print("â€¢ SprawdÅº funkcjÄ™ nagrody")
    else:
        print("â€¢ Model jest dobrze wytrenowany!")
        print("â€¢ MoÅ¼esz uÅ¼yÄ‡ go do dalszych eksperymentÃ³w")

def train_ppo_agent(env, args):
    """Trenowanie agenta PPO"""
    from agents.ppo_agent import PPOAgent
    from training.train_ppo import train_ppo
    from training.train_ppo_v2 import train_ppo_enhanced
    
    if args.continue_training:
        # Implementacja kontynuacji treningu PPO
        import glob
        actor_models = glob.glob('checkpoints/ppo/*_actor.keras')
        if actor_models:
            def extract_episode_number(filename):
                import re
                match = re.search(r'ep(\d+)', filename)
                return int(match.group(1)) if match else 0
            
            latest_actor = max(actor_models, key=extract_episode_number)
            latest_critic = latest_actor.replace('_actor.keras', '_critic.keras')
            
            print(f"Kontynuowanie z modeli: {latest_actor}, {latest_critic}")
            agent = PPOAgent.load(latest_actor, latest_critic, (84, 84, 1), 3)
        else:
            print("Nie znaleziono modeli PPO, rozpoczynanie nowego treningu...")
            agent = PPOAgent((84, 84, 1), 3)
    else:
        print("Rozpoczynanie nowego treningu PPO...")
        agent = PPOAgent((84, 84, 1), 3)
    
    # train_ppo(env, agent, episodes=args.episodes)
    train_ppo_enhanced(env, agent, episodes=args.episodes)

def test_ppo_agent(env, args):
    """Testowanie agenta PPO"""
    from agents.ppo_agent import PPOAgent
    from training.train_ppo import preprocess_state
    import time
    
    # Wczytaj model
    if args.model:
        actor_path = args.model
        critic_path = args.model.replace('_actor.keras', '_critic.keras')
        agent = PPOAgent.load(actor_path, critic_path, (84, 84, 1), 3)
    else:
        import glob
        actor_models = glob.glob('checkpoints/ppo/*_actor.keras')
        if actor_models:
            def extract_episode_number(filename):
                import re
                match = re.search(r'ep(\d+)', filename)
                return int(match.group(1)) if match else 0
            
            latest_actor = max(actor_models, key=extract_episode_number)
            latest_critic = latest_actor.replace('_actor.keras', '_critic.keras')
            
            print(f"Wczytywanie modeli: {latest_actor}, {latest_critic}")
            agent = PPOAgent.load(latest_actor, latest_critic, (84, 84, 1), 3)
        else:
            print("Nie znaleziono modeli PPO")
            return
    
    print(f"Model PPO: epizody treningowe={agent.episodes}")
    
    # Testowanie
    total_rewards = []
    completed_laps = 0
    
    for episode in range(args.episodes):
        print(f"\n=== EPIZOD {episode+1}/{args.episodes} ===")
        observation, info = env.reset()
        observation = preprocess_state(observation)
        
        episode_reward = 0
        steps = 0
        start_time = time.time()
        
        for step in range(1000):
            action = agent.act(observation)  # UÅ¼ywa deterministycznej polityki w testach
            next_observation, reward, terminated, truncated, info = env.step(action)
            next_observation = preprocess_state(next_observation)
            
            observation = next_observation
            episode_reward += reward
            steps += 1
            
            if steps % 100 == 0:
                print(f"  Krok {steps}, Nagroda: {episode_reward:.2f}")
            
            if terminated or truncated:
                break
        
        episode_time = time.time() - start_time
        total_rewards.append(episode_reward)
        
        # Ocena rezultatu
        if terminated and episode_reward > 600:
            completed_laps += 1
            result_text = "ğŸ† TOR UKOÅƒCZONY!"
        elif terminated and episode_reward > 300:
            result_text = "ğŸš— Dobra jazda - prawie ukoÅ„czyÅ‚!"
        else:
            result_text = "âŒ Nie ukoÅ„czyÅ‚ toru"
        
        print(f"Epizod {episode+1}: {steps} krokÃ³w, {episode_reward:.2f} pkt, {result_text}")
    
    # Podsumowanie
    print(f"\n=== PODSUMOWANIE PPO ===")
    print(f"Åšrednia nagroda: {np.mean(total_rewards):.2f}")
    print(f"UkoÅ„czone tory: {completed_laps}/{args.episodes}")
    print(f"WskaÅºnik sukcesu: {completed_laps/args.episodes*100:.1f}%")

  # Dla innych agentÃ³w
if __name__ == "__main__":
    main()