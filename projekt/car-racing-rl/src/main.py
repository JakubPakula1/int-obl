import argparse
import numpy as np
from environments.car_racing_env import CarRacingEnv
from agents.neat_agent import NEATAgent
from agents.dqn_agent import DQNAgent
from agents.random_agent import RandomAgent
from training.train_neat import train_neat, continue_from_checkpoint as continue_neat
from training.train_dqn import train_dqn, continue_from_checkpoint as continue_dqn
from evaluation.evaluate import evaluate
from training.train_ppo import train_ppo
import os
import time
import glob

def main():
    # Parsowanie argument√≥w wiersza polece≈Ñ
    parser = argparse.ArgumentParser(description='Car Racing RL')
    parser.add_argument('--agent', type=str, default='neat', 
                       choices=['neat', 'dqn', 'random', 'ppo', 'fuzzy', 'genetic', 'pso'],
                       help='Rodzaj agenta do trenowania/testowania')
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'],
                        help='Tryb pracy: trenowanie lub testowanie')
    parser.add_argument('--model', type=str, default=None,
                        help='≈öcie≈ºka do modelu do wczytania (dla trybu test)')
    parser.add_argument('--continue_training', action='store_true',
                        help='Kontynuuj trening z ostatniego checkpointu')
    parser.add_argument('--episodes', type=int, default=10,
                        help='Liczba epizod√≥w treningu/testowania')
    parser.add_argument('--max_steps', type=int, default=200,
                        help='Maksymalna liczba krok√≥w w epizodzie')
    parser.add_argument('--render', action='store_true',
                        help='Wy≈õwietlanie ≈õrodowiska w trakcie treningu')
    
    args = parser.parse_args()
    
    # Tworzenie ≈õrodowiska
    render_mode = "human" if args.render else None
    
    
    # Wyb√≥r agenta i trybu
    if args.mode == 'train':
        if args.agent == 'neat':
            env = CarRacingEnv(render_mode=render_mode)
            train_neat_agent(env, args)
        elif args.agent == 'dqn':
            env = CarRacingEnv(render_mode=render_mode, continuous=False)
            train_dqn_agent(env, args)
        elif args.agent == 'ppo':
            env = CarRacingEnv(render_mode=render_mode, continuous=True)  # PPO u≈ºywa ciƒÖg≈Çych akcji
            train_ppo_agent(env, args)
        elif args.agent == 'genetic':
            env = CarRacingEnv(render_mode=render_mode, continuous=True)
            train_genetic_agent(env, args)
        elif args.agent == 'pso':
            env = CarRacingEnv(render_mode=render_mode, continuous=True)
            train_pso_agent(env, args)
        elif args.agent == 'random':
            print("Agent losowy nie wymaga treningu")
    elif args.mode == 'test':
        if args.agent == 'neat':
            env = CarRacingEnv(render_mode=render_mode)
            test_neat_agent(env, args)
        elif args.agent == 'dqn':
            env = CarRacingEnv(render_mode=render_mode, continuous=False)
            test_dqn_agent(env, args)
        elif args.agent == 'ppo':
            env = CarRacingEnv(render_mode=render_mode, continuous=True)
            test_ppo_agent(env, args)
        elif args.agent == 'fuzzy':
            env = CarRacingEnv(render_mode=render_mode, continuous=True)
            test_fuzzy_agent(env, args)
        elif args.agent == 'genetic':
            env = CarRacingEnv(render_mode=render_mode, continuous=True)
            test_genetic_agent(env, args)
        elif args.agent == 'pso':
            env = CarRacingEnv(render_mode=render_mode, continuous=True)
            test_pso_agent(env, args)
        elif args.agent == 'random':
            env = CarRacingEnv(render_mode=render_mode, continuous=True)
            test_random_agent(env, args)
        
    
    env.close()

def train_neat_agent(env, args):
    if args.continue_training:
        # TYMCZASOWA POPRAWKA: U≈ºyj konkretnego checkpointu
        latest_checkpoint = 'checkpoints/neat-checkpoint-10'  # ‚Üê ZMIE≈É NA NAJNOWSZY
        
        if os.path.exists(latest_checkpoint):
            print(f"Kontynuowanie z checkpointu: {latest_checkpoint}")
            neat_agent, winner = continue_neat(latest_checkpoint, env, args.episodes)
        else:
            print("Szukam najnowszego checkpointu...")
            checkpoints = glob.glob('checkpoints/neat-checkpoint-*')
            if checkpoints:
                def extract_number(filename):
                    import re
                    match = re.search(r'checkpoint-(\d+)', filename)
                    return int(match.group(1)) if match else 0
                
                latest_checkpoint = max(checkpoints, key=extract_number)
                print(f"Kontynuowanie z checkpointu: {latest_checkpoint}")
                neat_agent, winner = continue_neat(latest_checkpoint, env, args.episodes)
            else:
                print("Nie znaleziono checkpoint√≥w, rozpoczynanie nowego treningu...")
                neat_agent, winner = train_neat(env, generations=args.episodes)
    else:
        print("Rozpoczynanie nowego treningu NEAT...")
        neat_agent, winner = train_neat(env, generations=args.episodes)

def train_dqn_agent(env, args):
    from models.neural_networks import DQNNetwork
    
    if args.continue_training:
        checkpoints = glob.glob('checkpoints/dqn/dqn_model_ep*.keras')
        if checkpoints:
            # Sortowanie numeryczne zamiast alfabetycznego
            def extract_episode_number(filename):
                import re
                match = re.search(r'ep(\d+)', filename)
                return int(match.group(1)) if match else 0
            
            latest_checkpoint = max(checkpoints, key=extract_episode_number)
            print(f"Kontynuowanie z checkpointu: {latest_checkpoint}")
            agent = DQNAgent.load(latest_checkpoint, (84,84,1), 5)
        else:
            print("Nie znaleziono checkpoint√≥w, rozpoczynanie nowego treningu...")
            model = DQNNetwork(input_shape=(84, 84, 1), action_space=5).model
            agent = DQNAgent((84, 84, 1), 5, model)
    else:
        print("Rozpoczynanie nowego treningu DQN...")
        model = DQNNetwork(input_shape=(84, 84, 1), action_space=5).model
        agent = DQNAgent((84, 84, 1), 5, model)
    
    # Trenowanie agenta DQN
    train_dqn(env, agent, episodes=args.episodes)

def test_neat_agent(env, args):
    """Testowanie agenta NEAT - POPRAWIONA WERSJA"""
    from agents.neat_agent import NEATAgent
    
    if args.model:
        print(f"Wczytywanie modelu: {args.model}")
        # POPRAWKA: U≈ºyj metody klasowej load_model
        neat_agent = NEATAgent.load_model(args.model, 'configs/neat_config.txt')
        if neat_agent is None:
            print("‚ùå Nie uda≈Ço siƒô wczytaƒá modelu!")
            return
    else:
        # Znajd≈∫ najnowszy model
        import glob
        models = glob.glob('models/neat_*.pkl')
        if models:
            latest_model = max(models, key=os.path.getctime)
            print(f"Wczytywanie najnowszego modelu: {latest_model}")
            neat_agent = NEATAgent.load_model(latest_model, 'configs/neat_config.txt')
        else:
            print("‚ùå Nie znaleziono modeli NEAT!")
            return
    
    print(f"üß¨ Model NEAT wczytany - fitness: {neat_agent.best_genome.fitness:.2f}")
    
    # Testowanie
    total_rewards = []
    
    for episode in range(args.episodes):
        print(f"\n=== EPIZOD {episode+1}/{args.episodes} ===")
        observation, info = env.reset()  # POPRAWKA: Prawid≈Çowy unpacking
        
        episode_reward = 0
        steps = 0
        start_time = time.time()
        
        for step in range(1000):
            action = neat_agent.act(observation)
            observation, reward, terminated, truncated, info = env.step(action)
            
            episode_reward += reward
            steps += 1
            
            if terminated or truncated:
                break
        
        total_rewards.append(episode_reward)
        episode_time = time.time() - start_time
        
        tiles_visited = info.get('tiles_visited', 0)
        print(f"Epizod {episode+1}: {episode_reward:.2f} pts, {steps} krok√≥w, {tiles_visited} p≈Çytek, {episode_time:.1f}s")
    
    # Statystyki
    avg_reward = np.mean(total_rewards)
    print(f"\nüéØ PODSUMOWANIE NEAT:")
    print(f"üìä ≈örednia nagroda: {avg_reward:.2f}")
    print(f"üèÜ Najlepszy wynik: {max(total_rewards):.2f}")
    print(f"üìâ Najgorszy wynik: {min(total_rewards):.2f}")


def test_dqn_agent(env, args):
    """Dedykowana funkcja testowania DQN z preprocessingiem"""
    from training.train_dqn import preprocess_state
    import time
    from environments.lap_completion_fix_wrapper import LapCompletionFixWrapper
    env = LapCompletionFixWrapper(env)
    # Wczytaj model
    if args.model:
        agent = DQNAgent.load(args.model, (84, 84, 1), 5)
    else:
        models = glob.glob('checkpoints/dqn/*.keras')
        if models:
            def extract_episode_number(filename):
                import re
                match = re.search(r'ep(\d+)', filename)
                return int(match.group(1)) if match else 0
            
            latest_model = max(models, key=extract_episode_number)
            print(f"Wczytywanie modelu: {latest_model}")
            agent = DQNAgent.load(latest_model, (84, 84, 1), 5)
        else:
            print("Nie znaleziono modelu DQN")
            return
    
    # Epsilon = 0 dla czystego testowania (tylko eksploatacja)
    agent.epsilon = 0.0
    print(f"Model: epsilon={agent.epsilon}, epizody treningowe={agent.episodes}")
    
    # Testowanie
    total_rewards = []
    total_steps = []
    completed_laps = 0
    
    for episode in range(args.episodes):
        print(f"\n=== EPIZOD {episode+1}/{args.episodes} ===")
        observation,info = env.reset()
        observation = preprocess_state(observation)
        
        episode_reward = 0
        steps = 0
        start_time = time.time()
        
        for step in range(1000):
            action = agent.act(observation)
            next_observation, reward, terminated, truncated, info = env.step(action)
            next_observation = preprocess_state(next_observation)
            
            observation = next_observation
            episode_reward += reward
            steps += 1
            
            # Wy≈õwietl progress co 100 krok√≥w
            if steps % 100 == 0:
                print(f"  Krok {steps}, Nagroda: {episode_reward:.2f}")
                print(info)
            
            if terminated or truncated:
                break
        
        episode_time = time.time() - start_time
        total_rewards.append(episode_reward)
        total_steps.append(steps)
        
        # Ocena rezultatu epizodu
        if terminated and episode_reward > 600:
            completed_laps += 1
            result_text = "üèÜ TOR UKO≈ÉCZONY!"
        elif terminated and episode_reward > 300:
            result_text = "üöó Dobra jazda - prawie uko≈Ñczy≈Ç!"
        elif terminated and episode_reward > 0:
            result_text = "‚úÖ Pozytywny wynik"
        elif terminated:
            result_text = "‚ùå Wypad≈Ç z toru"
        else:
            result_text = "‚è±Ô∏è Timeout - przekroczono limit czasu"
        
        print(f"Epizod {episode+1} uko≈Ñczony:")
        print(f"  Kroki: {steps}/1000")
        print(f"  Nagroda: {episode_reward:.2f}")
        print(f"  Czas: {episode_time:.2f}s")
        print(f"  Wynik: {result_text}")
        
        # Dodatkowe informacje o postƒôpie
        if episode_reward > 800:
            progress = "95-100%"
        elif episode_reward > 600:
            progress = "80-95%"
        elif episode_reward > 400:
            progress = "60-80%"
        elif episode_reward > 200:
            progress = "30-60%"
        elif episode_reward > 0:
            progress = "0-30%"
        else:
            progress = "Problemy z jazdƒÖ"
        
        print(f"  Szacowany postƒôp na torze: ~{progress}")
    
    # Podsumowanie ko≈Ñcowe
    print(f"\n{'='*50}")
    print(f"=== PODSUMOWANIE TEST√ìW DQN ===")
    print(f"{'='*50}")
    print(f"≈örednia nagroda: {np.mean(total_rewards):.2f} ¬± {np.std(total_rewards):.2f}")
    print(f"Najlepszy wynik: {max(total_rewards):.2f}")
    print(f"Najgorszy wynik: {min(total_rewards):.2f}")
    print(f"≈örednia liczba krok√≥w: {np.mean(total_steps):.1f}")
    print(f"Najd≈Çu≈ºszy epizod: {max(total_steps)} krok√≥w")
    print(f"Najkr√≥tszy epizod: {min(total_steps)} krok√≥w")
    
    # Analiza sukcesu
    print(f"\nAnaliza wynik√≥w:")
    successful_runs = sum(1 for r in total_rewards if r > 600)
    good_runs = sum(1 for r in total_rewards if r > 300)
    positive_runs = sum(1 for r in total_rewards if r > 0)
    
    print(f"Uko≈Ñczone tory (>600 pkt): {successful_runs}/{args.episodes} ({successful_runs/args.episodes*100:.1f}%)")
    print(f"Dobre wyniki (>300 pkt): {good_runs}/{args.episodes} ({good_runs/args.episodes*100:.1f}%)")
    print(f"Pozytywne wyniki (>0 pkt): {positive_runs}/{args.episodes} ({positive_runs/args.episodes*100:.1f}%)")
    
    # Ocena og√≥lna modelu
    print(f"\nOcena modelu:")
    avg_reward = np.mean(total_rewards)
    if successful_runs >= args.episodes * 0.8:
        print("üèÜ DOSKONA≈ÅY MODEL! Konsekwentnie uka≈Ñcza tory.")
    elif successful_runs >= args.episodes * 0.6:
        print("üåü BARDZO DOBRY MODEL! Czƒôsto uka≈Ñcza tory.")
    elif successful_runs >= args.episodes * 0.4:
        print("üëç DOBRY MODEL! Czasami uka≈Ñcza tory.")
    elif good_runs >= args.episodes * 0.6:
        print("‚ö° PRZYZWOITY MODEL! Dobrze je≈∫dzi, ale rzadko uka≈Ñcza.")
    elif positive_runs >= args.episodes * 0.6:
        print("üîÑ S≈ÅABY MODEL! Wymaga wiƒôcej treningu.")
    else:
        print("‚ùå BARDZO S≈ÅABY MODEL! Potrzebuje znacznie wiƒôcej treningu.")
    
    # Rekomendacje
    print(f"\nRekomendacje:")
    if successful_runs < args.episodes * 0.5:
        print("‚Ä¢ Kontynuuj trening modelu")
        print("‚Ä¢ Rozwa≈º dostrojenie hiperparametr√≥w")
        print("‚Ä¢ Sprawd≈∫ funkcjƒô nagrody")
    else:
        print("‚Ä¢ Model jest dobrze wytrenowany!")
        print("‚Ä¢ Mo≈ºesz u≈ºyƒá go do dalszych eksperyment√≥w")

def train_ppo_agent(env, args):
    """Trenowanie agenta PPO"""
    from agents.ppo_agent import PPOAgent
    from training.train_ppo import train_ppo
    
    if args.continue_training:
        # Implementacja kontynuacji treningu PPO
        import glob
        actor_models = glob.glob('checkpoints/ppo/*_actor.keras')
        if actor_models:
            def extract_episode_number(filename):
                import re
                match = re.search(r'ep(\d+)', filename)
                return int(match.group(1)) if match else 0
            
            latest_actor = max(actor_models, key=extract_episode_number)
            latest_critic = latest_actor.replace('_actor.keras', '_critic.keras')
            
            print(f"Kontynuowanie z modeli: {latest_actor}, {latest_critic}")
            agent = PPOAgent.load(latest_actor, latest_critic, (84, 84, 1), 3)
        else:
            print("Nie znaleziono modeli PPO, rozpoczynanie nowego treningu...")
            agent = PPOAgent((84, 84, 1), 3)
    else:
        print("Rozpoczynanie nowego treningu PPO...")
        agent = PPOAgent((84, 84, 1), 3)
    
    # train_ppo(env, agent, episodes=args.episodes)
    train_ppo(env, agent, episodes=args.episodes)

def test_ppo_agent(env, args):
    from stable_baselines3 import PPO
    import time
    from environments.lap_completion_fix_wrapper import LapCompletionFixWrapper
    import sys

    env = LapCompletionFixWrapper(env)
    model_path = "models/ppo_carracing1"

    if not os.path.exists(model_path + ".zip"):
        print(f"‚ùå Model nie znaleziony: {model_path}.zip")
        print("Dostƒôpne modele:")
        if os.path.exists("models/"):
            for file in os.listdir("models/"):
                if file.endswith(('.zip', '.pkl')):
                    print(f"  - {file}")
        sys.exit(1)

    try:
        model = PPO.load(model_path)
        print(f"‚úÖ Model wczytany: {model_path}")
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd wczytywania modelu: {e}")
        sys.exit(1)

    episodes = 5
    total_rewards = []

    print(f"üöÄ Rozpoczynanie ewaluacji PPO na {episodes} epizod√≥w")

    for ep in range(episodes):
        print(f"\n=== EPIZOD {ep + 1}/{episodes} ===")
        obs, info = env.reset()
        done = False
        total_reward = 0
        steps = 0

        while not done:
            action, _ = model.predict(obs, deterministic=True)  # deterministic=True dla test√≥w
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            steps += 1
            done = terminated or truncated
            
            # Opcjonalne spowolnienie dla lepszej wizualizacji
            time.sleep(1/60)
            
            # Zabezpieczenie przed niesko≈ÑczonƒÖ pƒôtlƒÖ
            if steps > 1000:
                print("‚è∞ Timeout - przerwano epizod")
                break

        total_rewards.append(total_reward)
        
        # Ocena wyniku
        if terminated and total_reward > 600:
            result = "üèÜ TOR UKO≈ÉCZONY!"
        elif terminated and total_reward > 300:
            result = "üöó Dobra jazda!"
        elif total_reward > 0:
            result = "‚úÖ Pozytywny wynik"
        else:
            result = "‚ùå S≈Çaby wynik"
        
        print(f"Epizod {ep + 1}: {steps} krok√≥w, {total_reward:.2f} pkt - {result}")
        
        # Informacje o postƒôpie na torze
        tiles_visited = info.get('tiles_visited', 0)
        total_tiles = info.get('total_tiles', 0)
        if total_tiles > 0:
            progress = (tiles_visited / total_tiles) * 100
            print(f"Postƒôp na torze: {tiles_visited}/{total_tiles} p≈Çytek ({progress:.1f}%)")

    # Podsumowanie
    print(f"\n{'='*50}")
    print(f"=== PODSUMOWANIE EWALUACJI PPO ===")
    print(f"{'='*50}")
    print(f"≈örednia nagroda: {sum(total_rewards)/len(total_rewards):.2f}")
    print(f"Najlepszy wynik: {max(total_rewards):.2f}")
    print(f"Najgorszy wynik: {min(total_rewards):.2f}")

    # Analiza sukcesu
    successful_runs = sum(1 for r in total_rewards if r > 600)
    good_runs = sum(1 for r in total_rewards if r > 300)
    positive_runs = sum(1 for r in total_rewards if r > 0)

    print(f"\nAnaliza wynik√≥w:")
    print(f"Uko≈Ñczone tory (>600 pkt): {successful_runs}/{episodes} ({successful_runs/episodes*100:.1f}%)")
    print(f"Dobre wyniki (>300 pkt): {good_runs}/{episodes} ({good_runs/episodes*100:.1f}%)")
    print(f"Pozytywne wyniki (>0 pkt): {positive_runs}/{episodes} ({positive_runs/episodes*100:.1f}%)")

    env.close()
    print("\n‚úÖ Ewaluacja zako≈Ñczona!")
def train_genetic_agent(env, args):
    """Trenowanie agenta genetycznego"""
    from agents.ga_agent import GeneticAgent
    
    agent = GeneticAgent(env, chromosome_length=600, population_size=30)
    agent.train(num_generations=args.episodes)

def train_pso_agent(env, args):
    """Trenowanie agenta PSO"""
    from agents.pso_agent import PSOAgent
    
    agent = PSOAgent(env, num_particles=20, dimensions=600)
    agent.train(max_iterations=args.episodes)

def test_fuzzy_agent(env, args):
    """Testowanie agenta rozmytego"""
    from agents.fuzzy_agent import FuzzyAgent
    
    agent = FuzzyAgent()
    
    total_rewards = []
    for episode in range(args.episodes):
        print(f"\n=== EPIZOD {episode+1}/{args.episodes} ===")
        observation, info = env.reset()
        episode_reward = 0
        steps = 0
        
        for step in range(1000):
            action = agent.act(observation)
            observation, reward, terminated, truncated, info = env.step(action)
            episode_reward += reward
            steps += 1
            
            if terminated or truncated:
                break
        
        total_rewards.append(episode_reward)
        print(f"Epizod {episode+1}: {steps} krok√≥w, {episode_reward:.2f} pkt")
    
    print(f"\nüß† PODSUMOWANIE FUZZY:")
    print(f"üìä ≈örednia nagroda: {np.mean(total_rewards):.2f}")

def test_genetic_agent(env, args):
    """Testowanie agenta genetycznego"""
    from agents.ga_agent import GeneticAgent
    
    if args.model:
        agent = GeneticAgent.load_best_genome(args.model, env)
    else:
        agent = GeneticAgent.load_best_genome('models/genetic_best_final.pkl', env)
    
    if agent is None:
        print("‚ùå Nie mo≈ºna wczytaƒá modelu genetycznego")
        return
    
    total_rewards = []
    for episode in range(args.episodes):
        print(f"\n=== EPIZOD {episode+1}/{args.episodes} ===")
        observation, info = env.reset()
        episode_reward = 0
        steps = 0
        
        for step in range(1000):
            action = agent.act(observation, step)
            observation, reward, terminated, truncated, info = env.step(action)
            episode_reward += reward
            steps += 1
            
            if terminated or truncated:
                break
        
        total_rewards.append(episode_reward)
        print(f"Epizod {episode+1}: {steps} krok√≥w, {episode_reward:.2f} pkt")
    
    print(f"\nüß¨ PODSUMOWANIE GENETIC:")
    print(f"üìä ≈örednia nagroda: {np.mean(total_rewards):.2f}")

def test_pso_agent(env, args):
    """Testowanie agenta PSO"""
    from agents.pso_agent import PSOAgent
    
    if args.model:
        agent = PSOAgent.load_model(args.model, env)
    else:
        agent = PSOAgent.load_model('models/pso_best_final.pkl', env)
    
    if agent is None:
        print("‚ùå Nie mo≈ºna wczytaƒá modelu PSO")
        return
    
    total_rewards = []
    for episode in range(args.episodes):
        print(f"\n=== EPIZOD {episode+1}/{args.episodes} ===")
        observation, info = env.reset()
        episode_reward = 0
        steps = 0
        
        for step in range(1000):
            action = agent.act(observation, step)
            observation, reward, terminated, truncated, info = env.step(action)
            episode_reward += reward
            steps += 1
            
            if terminated or truncated:
                break
        
        total_rewards.append(episode_reward)
        print(f"Epizod {episode+1}: {steps} krok√≥w, {episode_reward:.2f} pkt")
    
    print(f"\nüêù PODSUMOWANIE PSO:")
    print(f"üìä ≈örednia nagroda: {np.mean(total_rewards):.2f}")

def test_random_agent(env, args):
    """Testowanie agenta losowego - baseline"""
    from agents.random_agent import RandomAgent
    import time
    import numpy as np
    
    agent = RandomAgent()
    print(f"üé≤ Testowanie agenta losowego - {args.episodes} epizod√≥w")
    
    total_rewards = []
    total_steps = []
    completed_laps = 0
    
    for episode in range(args.episodes):
        print(f"\n=== EPIZOD {episode+1}/{args.episodes} ===")
        observation, info = env.reset()
        
        episode_reward = 0
        steps = 0
        start_time = time.time()
        
        for step in range(1000):
            # Agent losowy nie potrzebuje obserwacji - dzia≈Çania sƒÖ losowe
            action = agent.act()
            observation, reward, terminated, truncated, info = env.step(action)
            
            episode_reward += reward
            steps += 1
            
            # Wy≈õwietl progress co 200 krok√≥w
            if steps % 200 == 0:
                print(f"  Krok {steps}, Nagroda: {episode_reward:.2f}")
            
            if terminated or truncated:
                break
        
        episode_time = time.time() - start_time
        total_rewards.append(episode_reward)
        total_steps.append(steps)
        
        # Ocena rezultatu epizodu
        if terminated and episode_reward > 600:
            completed_laps += 1
            result_text = "üèÜ TOR UKO≈ÉCZONY! (niesamowite szczƒô≈õcie!)"
        elif terminated and episode_reward > 300:
            result_text = "üçÄ Bardzo szczƒô≈õliwy przejazd!"
        elif terminated and episode_reward > 100:
            result_text = "üé≤ Przyzwoity losowy wynik"
        elif terminated and episode_reward > 0:
            result_text = "‚úÖ Pozytywny wynik"
        elif terminated:
            result_text = "‚ùå Wypad≈Ç z toru (typowe dla losowego)"
        else:
            result_text = "‚è±Ô∏è Timeout - przekroczono limit czasu"
        
        print(f"Epizod {episode+1} uko≈Ñczony:")
        print(f"  Kroki: {steps}/1000")
        print(f"  Nagroda: {episode_reward:.2f}")
        print(f"  Czas: {episode_time:.2f}s")
        print(f"  Wynik: {result_text}")
    
    # Podsumowanie ko≈Ñcowe
    print(f"\n{'='*50}")
    print(f"=== PODSUMOWANIE TEST√ìW RANDOM ===")
    print(f"{'='*50}")
    
    try:
        mean_reward = sum(total_rewards) / len(total_rewards)
        std_reward = (sum((r - mean_reward)**2 for r in total_rewards) / len(total_rewards))**0.5
        mean_steps = sum(total_steps) / len(total_steps)
        
        print(f"≈örednia nagroda: {mean_reward:.2f} ¬± {std_reward:.2f}")
        print(f"Najlepszy wynik: {max(total_rewards):.2f}")
        print(f"Najgorszy wynik: {min(total_rewards):.2f}")
        print(f"≈örednia liczba krok√≥w: {mean_steps:.1f}")
        print(f"Najd≈Çu≈ºszy epizod: {max(total_steps)} krok√≥w")
        print(f"Najkr√≥tszy epizod: {min(total_steps)} krok√≥w")
        
        # Analiza sukcesu
        print(f"\nAnaliza wynik√≥w (baseline losowy):")
        successful_runs = sum(1 for r in total_rewards if r > 600)
        good_runs = sum(1 for r in total_rewards if r > 300)
        decent_runs = sum(1 for r in total_rewards if r > 100)
        positive_runs = sum(1 for r in total_rewards if r > 0)
        
        print(f"Uko≈Ñczone tory (>600 pkt): {successful_runs}/{args.episodes} ({successful_runs/args.episodes*100:.1f}%)")
        print(f"Bardzo dobre (>300 pkt): {good_runs}/{args.episodes} ({good_runs/args.episodes*100:.1f}%)")
        print(f"Przyzwoite (>100 pkt): {decent_runs}/{args.episodes} ({decent_runs/args.episodes*100:.1f}%)")
        print(f"Pozytywne wyniki (>0 pkt): {positive_runs}/{args.episodes} ({positive_runs/args.episodes*100:.1f}%)")
        
        # Ocena jako baseline
        print(f"\nüí° Analiza baseline:")
        if successful_runs >= 1:
            print("ü§Ø NIESPOTYKANE SZCZƒò≈öCIE! Agent losowy uko≈Ñczy≈Ç tor!")
        elif good_runs >= args.episodes * 0.1:
            print("üçÄ Niezwykle szczƒô≈õliwy agent losowy!")
        elif decent_runs >= args.episodes * 0.3:
            print("üé≤ Typowy agent losowy - czasami ma szczƒô≈õcie")
        elif positive_runs >= args.episodes * 0.5:
            print("üìä Normalny baseline - oko≈Ço po≈Çowy wynik√≥w pozytywnych")
        else:
            print("‚ùå Bardzo pechowy agent losowy")
        
        # Znaczenie jako baseline
        print(f"\nüìà Znaczenie jako baseline:")
        print(f"‚Ä¢ Ka≈ºdy inteligentny agent powinien osiƒÖgaƒá lepsze wyniki")
        print(f"‚Ä¢ ≈örednia nagroda {mean_reward:.2f} to minimum do pokonania")
        print(f"‚Ä¢ Wska≈∫nik sukcesu {successful_runs/args.episodes*100:.1f}% to pr√≥g referencyjny")
        
        if mean_reward > 0:
            print(f"‚Ä¢ ‚úÖ Baseline wydaje siƒô rozsƒÖdny")
        else:
            print(f"‚Ä¢ ‚ö†Ô∏è Bardzo niski baseline - ≈õrodowisko mo≈ºe byƒá trudne")
        
        return {
            'avg_reward': mean_reward,
            'success_rate': successful_runs/args.episodes*100,
            'total_rewards': total_rewards
        }
    
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd w obliczeniach: {e}")
        return None
  # Dla innych agent√≥w
if __name__ == "__main__":
    main()