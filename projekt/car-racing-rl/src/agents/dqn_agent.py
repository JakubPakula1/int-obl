import numpy as np
import random
import os
from collections import deque
from tensorflow.keras.models import load_model

class DQNAgent:
    
    def __init__(self, state_size, action_size, model, checkpoint_dir='checkpoints/dqn'):
        """
        Inicjalizacja agenta DQN
        
        Args:
            state_size: Rozmiar stanu (84, 84, 1) dla preprocessowanych obraz√≥w
            action_size: Liczba mo≈ºliwych akcji (5 dla CarRacing)
            model: Skompilowana sieƒá neuronowa (CNN)
            checkpoint_dir: Katalog do zapisywania modeli
        """
        # === ARCHITEKTURA SIECI ===
        self.state_size = state_size      # (84, 84, 1) - preprocessowany obraz
        self.action_size = action_size    # 5 akcji: nic, lewo, prawo, gaz, hamuj
        self.model = model                # G≈Ç√≥wna sieƒá Q-Network
        self.target_model = None          # Sieƒá docelowa (stabilizuje trening)
        
        # === EXPERIENCE REPLAY ===
        # Bufor cykliczny przechowujƒÖcy ostatnie 50k do≈õwiadcze≈Ñ (s,a,r,s',done)
        self.memory = deque(maxlen=50000)
        
        # === HYPERPARAMETRY Q-LEARNING ===
        self.gamma = 0.95                 # Wsp√≥≈Çczynnik dyskontowania (jak wa≈ºne sƒÖ nagrody w przysz≈Ço≈õci)
        self.epsilon = 1.0                # PoczƒÖtkowa eksploracja (100% losowych akcji)
        self.epsilon_min = 0.1            # Minimalna eksploracja (10% losowych akcji)
        self.epsilon_decay = 0.9995       # Tempo zmniejszania eksploracji (~0.05% co krok)
        self.batch_size = 32              # Rozmiar mini-batcha do treningu
        
        # === KONTROLA TRENINGU ===
        self.checkpoint_dir = checkpoint_dir
        self.steps = 0                    # Licznik krok√≥w treningu
        self.episodes = 0                 # Licznik uko≈Ñczonych epizod√≥w
        self.update_target_every = 10     # Aktualizuj target network co 10 trening√≥w
        self.train_every = 4              # Trenuj co 4 kroki (stabilno≈õƒá)
        
        # === INICJALIZACJA ===
        # Utw√≥rz katalog na checkpointy je≈õli nie istnieje
        import os
        if not os.path.exists(checkpoint_dir):
            os.makedirs(checkpoint_dir)
        
        # Stw√≥rz target model jako kopiƒô g≈Ç√≥wnego modelu
        self.update_target_model()
    
    def update_target_model(self):
        """
        Aktualizuje target network
        
        Target network to kopia g≈Ç√≥wnego modelu, kt√≥ra jest aktualizowana rzadziej.
        Zapobiega to niestabilno≈õci treningu, kt√≥ra wystƒôpuje gdy sieƒá uczy siƒô
        ze swoich w≈Çasnych predykcji.
        
        Mechanizm:
        1. G≈Ç√≥wna sieƒá: Q(s,a) - uczona na ka≈ºdym kroku
        2. Target sieƒá: Q_target(s',a') - aktualizowana co kilka krok√≥w
        3. Loss = [r + Œ≥ * max(Q_target(s',a')) - Q(s,a)]¬≤
        """
        if self.target_model is None:
            # Pierwsza inicjalizacja - sklonuj architekturƒô
            from tensorflow.keras.models import clone_model
            self.target_model = clone_model(self.model)
            self.target_model.compile(loss='mse', optimizer='adam')
        
        # Skopiuj wagi z g≈Ç√≥wnego modelu do target model
        self.target_model.set_weights(self.model.get_weights())
        print(f"üéØ Target model zaktualizowany (krok {self.steps})")
    
    def act(self, state):
        """
        Wybiera akcjƒô na podstawie stanu (Epsilon-Greedy Strategy)
        
        Args:
            state: Preprocessowany stan ≈õrodowiska (84, 84, 1)
            
        Returns:
            action: Indeks akcji (0-4)
            
        
        Na poczƒÖtku Œµ=1.0 (100% eksploracji), ko≈Ñczy na Œµ=0.1 (10% eksploracji)
        """
        if np.random.rand() <= self.epsilon:
            # EKSPLORACJA: wybierz losowƒÖ akcjƒô
            action = np.random.randint(0, self.action_size)
            return action
        
        # EKSPLOATACJA: wybierz najlepszƒÖ akcjƒô wed≈Çug Q-sieci
        # Dodaj batch dimension (1, 84, 84, 1) - TensorFlow wymaga batch
        state_batch = np.expand_dims(state, axis=0) 
        
        # Oblicz Q-values dla wszystkich akcji
        q_values = self.model.predict(state_batch, verbose=0)  # Shape: (1, 5)
        
        # Zwr√≥ƒá akcjƒô z najwy≈ºszƒÖ Q-warto≈õciƒÖ
        return np.argmax(q_values[0])

    def train(self, state, action, reward, next_state, done):
        """
        G≈Ç√≥wna funkcja treningu - zapisuje do≈õwiadczenie i uczy sieƒá
        
        Args:
            state: Stan przed akcjƒÖ
            action: Wykonana akcja (0-4)
            reward: Otrzymana nagroda
            next_state: Stan po akcji
            done: Czy epizod siƒô sko≈Ñczy≈Ç
            
        Proces:
        1. Zapisz przej≈õcie (s,a,r,s',done) w experience replay buffer
        2. Co 4 kroki: trenuj sieƒá na losowym batch'u z bufora
        3. Co 10 trening√≥w: aktualizuj target network
        4. Co 5 epizod√≥w: zapisz checkpoint modelu
        """
        # === 1. ZAPISZ DO≈öWIADCZENIE ===
        # Experience tuple: (stan, akcja, nagroda, nastƒôpny_stan, czy_koniec)
        self.memory.append((state, action, reward, next_state, done))
        self.steps += 1
        
        # === 2. TRENING CO 4 KROKI ===
        # Trenuj tylko gdy mamy wystarczajƒÖco do≈õwiadcze≈Ñ i co train_every krok√≥w
        if len(self.memory) > self.batch_size and self.steps % self.train_every == 0:
            # Trenuj na losowym batch'u z experience replay
            self.replay(self.batch_size)
            
            # === 3. AKTUALIZACJA TARGET NETWORK ===
            # Co update_target_every trening√≥w skopiuj wagi do target model
            if self.steps % self.update_target_every == 0:
                self.update_target_model()
        
        # === 4. MONITORING I LOGI ===
        # Wy≈õwietl progress co 100 krok√≥w
        if self.steps % 100 == 0:
            print(f"    üîÑ Krok: {self.steps}, Epsilon: {self.epsilon:.4f}, "
                  f"Pamiƒôƒá: {len(self.memory)}/{self.memory.maxlen}")

        # === 5. CHECKPOINT PO EPIZODZIE ===
        # Je≈õli epizod siƒô ko≈Ñczy, zwiƒôksz licznik i zapisz model co 5 epizod√≥w
        if done:
            self.episodes += 1
            if self.episodes % 5 == 0:
                self.save_model()
                print(f"üíæ Checkpoint: Model zapisany po {self.episodes} epizodach")
    
    def replay(self, batch_size):
        """
        Experience Replay - uczy sieƒá na losowym batch'u przesz≈Çych do≈õwiadcze≈Ñ
        
        Args:
            batch_size: Rozmiar mini-batcha (32)
            
        Algorytm Q-Learning:
        1. Pobierz losowy batch do≈õwiadcze≈Ñ z bufora
        2. Oblicz Q(s,a) dla obecnych stan√≥w (g≈Ç√≥wna sieƒá)
        3. Oblicz Q(s',a') dla nastƒôpnych stan√≥w (target sieƒá)
        4. Oblicz target: Q_target = r + Œ≥ * max(Q_target(s',a'))
        5. Minimalizuj b≈ÇƒÖd: Loss = [Q_target - Q(s,a)]¬≤
        6. Zmniejsz epsilon (mniej eksploracji z czasem)
        """
        # === 1. LOSOWY BATCH Z EXPERIENCE REPLAY ===
        minibatch = random.sample(self.memory, batch_size)
        
        # === 2. PRZYGOTOWANIE DANYCH WSADOWYCH ===
        # Zamiast 32 pojedynczych predykcji, robimy 2 batch'e (ZNACZNIE SZYBSZE!)
        states = np.array([transition[0] for transition in minibatch])        # (32, 84, 84, 1)
        actions = np.array([transition[1] for transition in minibatch])       # (32,)
        rewards = np.array([transition[2] for transition in minibatch])       # (32,)
        next_states = np.array([transition[3] for transition in minibatch])   # (32, 84, 84, 1)
        dones = np.array([transition[4] for transition in minibatch])         # (32,)
        
        # === 3. OBLICZ Q-VALUES ===
        # TYLKO 2 wywo≈Çania predict zamiast 64! (batch processing)
        current_q_values = self.model.predict(states, verbose=0)         # Q(s,a) - (32, 5)
        next_q_values = self.target_model.predict(next_states, verbose=0) # Q_target(s',a') - (32, 5)
        
        # === 4. BELLMAN EQUATION ===
        # Skopiuj obecne Q-values (bƒôdziemy modyfikowaƒá tylko wybranƒÖ akcjƒô)
        target_q_values = current_q_values.copy()
        
        for i in range(batch_size):
            if dones[i]:
                # Stan terminalny: Q_target = r (brak przysz≈Çych nagr√≥d)
                target_q_values[i][actions[i]] = rewards[i]
            else:
                # Stan nieterminalny: Q_target = r + Œ≥ * max(Q_target(s',a'))
                target_q_values[i][actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])
        
        # === 5. TRENING SIECI ===
        # Minimalizuj b≈ÇƒÖd miƒôdzy Q(s,a) a Q_target
        # Loss = MSE([target_q_values - current_q_values])
        self.model.fit(states, target_q_values, epochs=1, verbose=0)
        
        # === 6. ZMNIEJSZ EKSPLORACJƒò ===
        # Z czasem zmniejszaj epsilon: agent przechodzi z eksploracji do eksploatacji
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
            # Po ~1400 krokach epsilon spadnie z 1.0 do ~0.1

    def save_model(self, custom_name=None):
        """
        Zapisuje model wraz z parametrami treningu (checkpointing)
        
        Args:
            custom_name: Opcjonalna nazwa pliku (domy≈õlnie: dqn_model_ep{episodes})
            
        Zapisuje:
        1. Model .keras - architektura sieci + wagi
        2. Parametry .json - stan treningu (episodes, steps, epsilon, itp.)
        
        Dziƒôki temu mo≈ºna wznowiƒá trening od dowolnego punktu!
        """
        try:
            # === ≈öCIE≈ªKI PLIK√ìW ===
            if custom_name:
                filepath = os.path.join(self.checkpoint_dir, f"{custom_name}.keras")
                params_filepath = os.path.join(self.checkpoint_dir, f"{custom_name}_params.json")
            else:
                filepath = os.path.join(self.checkpoint_dir, f"dqn_model_ep{self.episodes}.keras")
                params_filepath = os.path.join(self.checkpoint_dir, f"dqn_model_ep{self.episodes}_params.json")
            
            # === ZAPISZ MODEL ===
            # Keras format - zawiera architekturƒô + wagi + optimizer state
            self.model.save(filepath)
            
            # === ZAPISZ PARAMETRY TRENINGU ===
            import json
            params = {
                'episodes': self.episodes,      # Ile epizod√≥w przeszed≈Ç trening
                'steps': self.steps,            # Ile krok√≥w treningu wykonano
                'epsilon': self.epsilon,        # Obecny poziom eksploracji
                'memory_size': len(self.memory) # Ile do≈õwiadcze≈Ñ w buforze
            }
            with open(params_filepath, 'w') as f:
                json.dump(params, f, indent=2)
                
            print(f"üíæ Model zapisany: {filepath}")
            print(f"üìä Parametry: eps={self.episodes}, steps={self.steps}, Œµ={self.epsilon:.4f}")
            
        except Exception as e:
            print(f"‚ùå B≈ÅƒÑD podczas zapisywania: {e}")
    
    @classmethod
    def load(cls, model_path, state_size, action_size):
        """
        ≈Åaduje zapisany model i przywraca stan treningu (resumable training)
        
        Args:
            model_path: ≈öcie≈ºka do pliku .keras
            state_size: Rozmiar stanu (84, 84, 1)
            action_size: Liczba akcji (5)
            
        Returns:
            DQNAgent: Agent z wczytanym modelem i przywr√≥conymi parametrami
            
        Proces:
        1. Wczytaj model .keras (architektura + wagi)
        2. Stw√≥rz agenta z tym modelem
        3. Przywr√≥ƒá parametry treningu z .json (epsilon, episodes, itp.)
        4. Zrekonstruuj target model
        """
        try:
            # === 1. WCZYTAJ MODEL ===
            from tensorflow.keras.models import load_model
            import json
            
            print(f"üìÇ ≈Åadowanie modelu z: {model_path}")
            model = load_model(model_path)
            
            # === 2. STW√ìRZ AGENTA ===
            agent = cls(state_size, action_size, model)
            
            # === 3. PRZYWR√ìƒÜ PARAMETRY TRENINGU ===
            params_path = model_path.replace('.keras', '_params.json')
            if os.path.exists(params_path):
                try:
                    with open(params_path, 'r') as f:
                        params = json.load(f)
                    
                    # Przywr√≥ƒá stan treningu
                    agent.episodes = params.get('episodes', 0)
                    agent.steps = params.get('steps', 0)
                    agent.epsilon = params.get('epsilon', 1.0)
                    
                    print(f"‚úÖ Model wczytany pomy≈õlnie!")
                    print(f"üìä Przywr√≥cone: epizody={agent.episodes}, "
                          f"kroki={agent.steps}, Œµ={agent.epsilon:.4f}")
                          
                except Exception as e:
                    print(f"‚ö†Ô∏è Nie uda≈Ço siƒô wczytaƒá parametr√≥w: {e}")
                    print("üîÑ U≈ºywam domy≈õlnych parametr√≥w treningu")
            else:
                print(f"‚ö†Ô∏è Brak pliku parametr√≥w: {params_path}")
                print("üîÑ U≈ºywam domy≈õlnych parametr√≥w")
            
            return agent
            
        except Exception as e:
            print(f"‚ùå B≈ÅƒÑD podczas ≈Çadowania modelu: {e}")
            raise