import numpy as np
import time
import os
import glob
import cv2

def preprocess_state(state):
    """
    Bardzo szybki preprocessing obrazu dla DQN
    
    Args:
        state: Surowy obraz z Å›rodowiska CarRacing (96x96x3 RGB)
        
    Returns:
        processed_state: Preprocessowany obraz (84x84x1)
        
    Transformacje:
    1. RGB â†’ Grayscale (3 kanaÅ‚y â†’ 1 kanaÅ‚, 3x szybsze)
    2. 96x96 â†’ 84x84
    3. [0-255] â†’ [0-1] (normalizacja dla sieci neuronowej)
    4. Reshape do formatu TensorFlow (height, width, channels)

    - Mniejszy obraz = szybszy trening
    """
    # === 1. KONWERSJA RGB â†’ GRAYSCALE ===
    # Zmniejszamy z 3 kanaÅ‚Ã³w (RGB) do 1 kanaÅ‚u (grayscale)
    gray = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)  # (96,96,3) â†’ (96,96)
    
    # === 2. ZMIANA ROZMIARU ===

    resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)  # (96,96) â†’ (84,84)
    
    # === 3. NORMALIZACJA ===
    # Piksele z [0-255] â†’ [0-1] (standard dla sieci neuronowych)
    # Dzielenie przez 255.0 (float) zapewnia wartoÅ›ci zmiennoprzecinkowe
    normalized = resized / 255.0  # (84,84) z wartoÅ›ciami [0,1]
    
    # === 4. RESHAPE DLA TENSORFLOW ===
    # TensorFlow CNN oczekuje (height, width, channels)
    # Dodajemy wymiar kanaÅ‚u: (84,84) â†’ (84,84,1)
    return normalized.reshape(84, 84, 1)

def train_dqn(env, agent, episodes=10, max_steps=1000):
    """
    GÅ‚Ã³wna pÄ™tla treningu DQN/DDQN
    
    Args:
        env: Åšrodowisko CarRacing
        agent: Agent DQN z zaÅ‚adowanym modelem
        episodes: Liczba epizodÃ³w treningu
        max_steps: Maksymalna liczba krokÃ³w na epizod (zapobiega nieskoÅ„czonym pÄ™tlom)
    
    Proces treningu:
    1. Reset Å›rodowiska â†’ nowy stan poczÄ…tkowy
    2. Dla kaÅ¼dego kroku: action â†’ step â†’ train â†’ repeat
    3. Zbieranie statystyk i monitoring postÄ™pu
    4. Automatyczne zapisywanie co 5 epizodÃ³w (w DQNAgent)
    """
    # === INICJALIZACJA TRENINGU ===
    total_rewards = []           # Historia nagrÃ³d ze wszystkich epizodÃ³w
    start_time = time.time()     # Timer do pomiaru czasu treningu
    
    print(f"ğŸš€ Rozpoczynanie treningu DQN: {episodes} epizodÃ³w, max {max_steps} krokÃ³w/epizod")
    
    # === GÅÃ“WNA PÄ˜TLA EPIZODÃ“W ===
    for episode in range(1, episodes + 1):
        print(f"\n=== EPIZOD {episode}/{episodes} ===")
        
        # === 1. RESET ÅšRODOWISKA ===
        # KaÅ¼dy epizod zaczyna siÄ™ od nowego, losowego stanu poczÄ…tkowego
        state = env.reset()
        print(f"ğŸ“ Stan poczÄ…tkowy wczytany (ksztaÅ‚t: {state.shape})")
        
        # Preprocessing: (96,96,3) â†’ (84,84,1)
        state = preprocess_state(state)
        
        # Zmienne epizodu
        total_reward = 0    # Suma nagrÃ³d w tym epizodzie
        steps = 0          # Licznik krokÃ³w w epizodzie
        
        # === 2. PÄ˜TLA KROKÃ“W W EPIZODZIE ===
        for step in range(max_steps):
            # === MONITORING POSTÄ˜PU ===
            # PokaÅ¼ progress co 50 krokÃ³w (Å¼eby nie zaÅ›miecaÄ‡ konsoli)
            if step % 50 == 0:
                elapsed = time.time() - start_time
                print(f"  ğŸ”„ Krok {step}/{max_steps}, Czas treningu: {elapsed:.2f}s, "
                      f"Nagroda: {total_reward:.2f}, Îµ: {agent.epsilon:.4f}")
            
            # === 3. WYBIERZ AKCJÄ˜ ===
            # Agent uÅ¼ywa epsilon-greedy: losowa vs najlepsza akcja
            action = agent.act(state)  # 0-4 (nic, lewo, prawo, gaz, hamuj)
            
            # === 4. WYKONAJ KROK W ÅšRODOWISKU ===
            # CarRacing zwraca: nastÄ™pny_stan, nagroda, czy_skoÅ„czony, czy_przerwany, info
            next_state, reward, terminated, truncated, _ = env.step(action)
            
            # Preprocessing nastÄ™pnego stanu
            next_state = preprocess_state(next_state)
            
            # === 5. OKREÅšL CZY EPIZOD SIÄ˜ KOÅƒCZY ===
            # Epizod koÅ„czy siÄ™ gdy:
            # - terminated: agent osiÄ…gnÄ…Å‚ cel lub wyleciaÅ‚ z toru
            # - truncated: przekroczono limit czasu Å›rodowiska
            # - step == max_steps-1: nasz limit bezpieczeÅ„stwa
            done = terminated or truncated or (step == max_steps - 1)
            
            # === 6. TRENUJ AGENTA ===
            # Zapisz doÅ›wiadczenie (s,a,r,s',done) i potencjalnie trenuj sieÄ‡
            agent.train(state, action, reward, next_state, done)
            
            # === 7. PRZEJDÅ¹ DO NASTÄ˜PNEGO STANU ===
            state = next_state
            total_reward += reward  # Akumuluj nagrody
            steps += 1
            
            # === 8. SPRAWDÅ¹ WARUNKI KOÅƒCA EPIZODU ===
            if terminated or truncated:
                reason = "ğŸ Terminated" if terminated else "â° Truncated"
                print(f"  {reason} po {steps} krokach")
                break
        
        # === STATYSTYKI EPIZODU ===
        total_rewards.append(total_reward)
        
        # Åšrednia z ostatnich 10 epizodÃ³w (rolling average)
        recent_rewards = total_rewards[-min(len(total_rewards), 10):]
        avg_reward = np.mean(recent_rewards)
        
        # === RAPORT POSTÄ˜PU ===
        elapsed_time = time.time() - start_time
        print(f"âœ… Epizod {episode} ukoÅ„czony:")
        print(f"   Nagroda: {total_reward:.2f}")
        print(f"   Åšrednia (10 ep): {avg_reward:.2f}")
        print(f"   Kroki: {steps}/{max_steps}")
        print(f"   Czas treningu: {elapsed_time:.2f}s")
        print(f"   Epsilon: {agent.epsilon:.4f}")
        print(f"   PamiÄ™Ä‡: {len(agent.memory)}/{agent.memory.maxlen}")
        
        # === OCENA POSTÄ˜PU ===
        # JakoÅ›ciowa ocena jak agent sobie radzi
        if total_reward > 600:
            performance = "ğŸ† DOSKONALE! (ukoÅ„czyÅ‚ tor)"
        elif total_reward > 300:
            performance = "ğŸš— DOBRZE (solidna jazda)"
        elif total_reward > 100:
            performance = "ğŸ¯ ÅšREDNIO (pewien postÄ™p)"
        elif total_reward > 0:
            performance = "ğŸ¤” SÅABO (ale pozytywnie)"
        else:
            performance = "âŒ BARDZO SÅABO (nagroda ujemna)"
        
        print(f"   Ocena: {performance}")
        
        # === ANALIZA TRENDU ===
        # SprawdÅº czy agent siÄ™ poprawia
        if len(total_rewards) >= 5:
            recent_avg = np.mean(total_rewards[-5:])    # Ostatnie 5
            older_avg = np.mean(total_rewards[-10:-5])   # Poprzednie 5
            
            if len(total_rewards) >= 10:
                if recent_avg > older_avg:
                    trend = "ğŸ“ˆ POPRAWA"
                elif recent_avg < older_avg - 50:
                    trend = "ğŸ“‰ POGORSZENIE"
                else:
                    trend = "â¡ï¸ STABILNY"
                print(f"   Trend: {trend}")
    
    # === KOÅƒCOWY ZAPIS ===
    print(f"\n{'='*50}")
    print("ğŸ¯ TRENING ZAKOÅƒCZONY")
    print("="*50)
    print(f"ÅÄ…czny czas treningu: {time.time() - start_time:.2f}s")
    print(f"Ostatnia nagroda: {total_rewards[-1]:.2f}")
    print(f"Najlepsza nagroda: {max(total_rewards):.2f}")
    print(f"Åšrednia wszystkich: {np.mean(total_rewards):.2f}")
    
    print("ğŸ’¾ Zapisywanie koÅ„cowego modelu...")
    try:
        # Zapisz model jako "dqn_final_model" (niezaleÅ¼nie od liczby epizodÃ³w)
        agent.save_model(custom_name="dqn_final_model")
        print("âœ… Model koÅ„cowy zapisany!")
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d podczas zapisywania koÅ„cowego modelu: {e}")
        import traceback
        traceback.print_exc()
    
    return total_rewards, agent

def continue_from_checkpoint(checkpoint_path, env, episodes=10):
    """
    Kontynuuje trening z zapisanego checkpointu (Resumable Training)
    
    Args:
        checkpoint_path: ÅšcieÅ¼ka do zapisanego modelu (.keras)
        env: Åšrodowisko CarRacing
        episodes: Ile dodatkowych epizodÃ³w trenowaÄ‡
    
    Returns:
        total_rewards: Historia nagrÃ³d z kontynuowanego treningu
        agent: Wytrenowany agent
    
    Proces:
    1. Wczytaj zapisany model + parametry treningu
    2. PrzywrÃ³Ä‡ stan agenta (epsilon, episodes, memory)
    3. Kontynuuj trening od punktu przerwania
    
    UÅ¼ycie:
    >> rewards, agent = continue_from_checkpoint("checkpoints/dqn/dqn_model_ep50.keras", env, 20)
    >> # Kontynuuje trening od epizodu 50, dodajÄ…c 20 epizodÃ³w wiÄ™cej
    """
    print(f"ğŸ”„ KONTYNUACJA TRENINGU Z CHECKPOINTU")
    print(f"ğŸ“‚ ÅšcieÅ¼ka: {checkpoint_path}")
    
    try:
        # === 1. WCZYTAJ AGENTA Z CHECKPOINTU ===
        from agents.dqn_agent import DQNAgent
        
        # DQNAgent.load() automatycznie:
        # - Åaduje model (.keras) 
        # - Przywraca parametry treningu (_params.json)
        # - Rekonstruuje target_model
        # - Ustawia epsilon, episodes, steps na zapisane wartoÅ›ci
        agent = DQNAgent.load(checkpoint_path, (84,84,1), 5)
        
        print(f"âœ… Agent wczytany z {agent.episodes} epizodÃ³w treningu")
        print(f"ğŸ“Š Stan: steps={agent.steps}, Îµ={agent.epsilon:.4f}")
        
        # === 2. KONTYNUUJ TRENING ===
        # UÅ¼ywaj tej samej funkcji train_dqn, ale agent pamiÄ™ta swÃ³j stan
        print(f"ğŸš€ Kontynuacja treningu na {episodes} dodatkowych epizodÃ³w...")
        
        total_rewards, trained_agent = train_dqn(env, agent, episodes)
        
        print(f"âœ… Trening kontynuowany! ÅÄ…cznie epizodÃ³w: {trained_agent.episodes}")
        return total_rewards, trained_agent
        
    except Exception as e:
        print(f"âŒ BÅÄ„D podczas kontynuacji treningu: {e}")
        import traceback
        traceback.print_exc()
        raise