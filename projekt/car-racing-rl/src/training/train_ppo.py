import numpy as np
import time
import cv2

def preprocess_state(state):
    """Preprocessing obrazu dla PPO"""
    gray = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)
    resized = cv2.resize(gray, (84, 84))
    normalized = resized / 255.0
    return normalized.reshape(84, 84, 1)

def train_ppo(env, agent, episodes=100, max_steps=1000):
    """Trenowanie agenta PPO - poprawiona wersja"""
    total_rewards = []
    start_time = time.time()
    
    print(f"ğŸš€ Rozpoczynanie treningu PPO na {episodes} epizodÃ³w")
    
    for episode in range(1, episodes + 1):
        observation, info = env.reset()
        state = preprocess_state(observation)
        episode_reward = 0
        steps = 0
        
        episode_start_time = time.time()
        
        for step in range(max_steps):
            # Pobierz akcjÄ™ i wartoÅ›Ä‡ stanu
            action, log_prob = agent.get_action(state)
            state_batch = np.expand_dims(state, axis=0)
            value = agent.critic(state_batch, training=False).numpy()[0][0]
            
            # Wykonaj akcjÄ™ w Å›rodowisku
            next_observation, reward, terminated, truncated, info = env.step(action)
            next_state = preprocess_state(next_observation)
            
            done = terminated or truncated or (step == max_steps - 1)
            
            # Zapisz przejÅ›cie
            agent.store_transition(state, action, reward, value, log_prob, done)
            
            state = next_state
            episode_reward += reward
            steps += 1
            
            # WAÅ»NE: SprawdÅº czy trzeba wykonaÄ‡ aktualizacjÄ™
            if agent.should_update():
                print(f"ğŸ“Š Wykonywanie aktualizacji PPO po {len(agent.states)} krokach...")
                agent.update()
            
            if terminated or truncated:
                break
        
        # ZakoÅ„czenie epizodu
        agent.episodes += 1
        episode_time = time.time() - episode_start_time
        
        # Statystyki epizodu
        total_rewards.append(episode_reward)
        avg_reward = np.mean(total_rewards[-min(len(total_rewards), 10):])
        
        # Analiza rezultatu
        if terminated and episode_reward > 500:
            result = "ğŸ† DOSKONAÅY WYNIK!"
        elif terminated and episode_reward > 200:
            result = "ğŸš— DOBRY WYNIK!"
        elif terminated and episode_reward > 0:
            result = "âœ… POZYTYWNY WYNIK"
        elif episode_reward > -50:
            result = "âš ï¸ SÅABY WYNIK"
        else:
            result = "âŒ BARDZO SÅABY WYNIK"
        
        print(f"Epizod {episode}/{episodes}: {steps} krokÃ³w, {episode_reward:.2f} pkt, "
              f"Å›rednia: {avg_reward:.2f}, czas: {episode_time:.1f}s - {result}")
        
        # Zapisz model co 10 epizodÃ³w
        if agent.episodes % 10 == 0:
            agent.save_model()
            print(f"ğŸ’¾ Model zapisany po epizodzie {agent.episodes}")
    
    # Finalna aktualizacja jeÅ›li sÄ… pozostaÅ‚e dane
    if len(agent.states) > 0:
        print("ğŸ“Š Finalna aktualizacja PPO...")
        agent.update()
    
    # Zapisz finalny model
    agent.save_model(custom_name="ppo_final_model")
    
    total_time = time.time() - start_time
    print(f"\nğŸ¯ Trening PPO zakoÅ„czony!")
    print(f"â±ï¸ CaÅ‚kowity czas: {total_time:.2f}s")
    print(f"ğŸ“ˆ Åšrednia nagroda: {np.mean(total_rewards):.2f}")
    print(f"ğŸ† Najlepszy wynik: {max(total_rewards):.2f}")
    
    return agent, total_rewards