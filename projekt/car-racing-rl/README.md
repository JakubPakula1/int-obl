# ğŸï¸ Car Racing RL - PorÃ³wnanie AlgorytmÃ³w Uczenia ze Wzmocnieniem

Kompleksowy projekt porÃ³wnujÄ…cy skutecznoÅ›Ä‡ rÃ³Å¼nych algorytmÃ³w uczenia ze wzmocnieniem (RL) w Å›rodowisku **CarRacing-v3** z biblioteki Gymnasium. Implementuje i porÃ³wnuje **4 rÃ³Å¼ne podejÅ›cia**: Deep Q-Network (DQN), Proximal Policy Optimization (PPO), NeuroEvolution of Augmenting Topologies (NEAT) oraz baseline Random Agent.

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![Gymnasium](https://img.shields.io/badge/gymnasium-0.29+-green.svg)
![TensorFlow](https://img.shields.io/badge/tensorflow-2.0+-orange.svg)
![Status](https://img.shields.io/badge/status-completed-success.svg)

## ğŸ¯ Cel Projektu

Zbadanie i porÃ³wnanie rÃ³Å¼nych algorytmÃ³w RL pod kÄ…tem:

- **SkutecznoÅ›ci uczenia** - jak szybko agent siÄ™ uczy
- **WydajnoÅ›ci koÅ„cowej** - jak dobrze nauczony agent jeÅºdzi
- **StabilnoÅ›ci treningu** - czy wyniki sÄ… powtarzalne
- **KompleksowoÅ›ci implementacji** - trudnoÅ›Ä‡ wdroÅ¼enia

## ğŸ† Wyniki EksperymentÃ³w

### ğŸ“Š Ranking AgentÃ³w (30 epizodÃ³w testowych)

| ğŸ¥‡ Pozycja | Agent      | Åšrednia Nagroda   | WskaÅºnik UkoÅ„czenia | Ocena        |
| ---------- | ---------- | ----------------- | ------------------- | ------------ |
| **1st** ğŸ† | **DQN**    | **844.6 Â± 45.2**  | **81%**             | DOSKONAÅY    |
| **2nd** ğŸ¥ˆ | **PPO**    | **623.4 Â± 78.9**  | **63%**             | BARDZO DOBRY |
| **3rd** ğŸ¥‰ | **NEAT**   | **387.2 Â± 156.3** | **32%**             | DOBRY        |
| **4th** âŒ | **Random** | **-42.1 Â± 23.7**  | **0%**              | SÅABY        |

### ğŸ¨ Wizualizacja WynikÃ³w

```
Åšrednie Nagrody:
DQN     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 844.6
PPO     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           623.4
NEAT    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         387.2
Random  â–Œ                                            -42.1
        0    200   400   600   800   1000
```

## ğŸ“Š Wykresy

### ğŸ† Wykres PorÃ³wnawczy GÅ‚Ã³wny

![PorÃ³wnanie AgentÃ³w](plots/direct_test_comparison_20250618_095237.png)

### ğŸ“ˆ SzczegÃ³Å‚owe Analizy AgentÃ³w

#### ğŸ§  DQN - SzczegÃ³Å‚owa Analiza

![DQN Analysis](plots/direct_test_results_100_dqn_analysis_20250618_112336.png)

#### ğŸ­ PPO - SzczegÃ³Å‚owa Analiza

![PPO Analysis](plots/direct_test_results_100_ppo_analysis_20250618_112338.png)

#### ğŸ§¬ NEAT - SzczegÃ³Å‚owa Analiza

![NEAT Analysis](plots/direct_test_results_100_neat_analysis_20250618_112337.png)

#### ğŸ² Random Agent - Baseline

![Random Analysis](plots/direct_test_results_100_random_analysis_20250618_112339.png)

### ğŸ“ˆ SzczegÃ³Å‚owa Analiza

#### ğŸ§  **DQN (Double Deep Q-Network)**

- **âœ… Mocne strony:**

  - NajwyÅ¼sza Å›rednia nagroda (844.6 punktÃ³w)
  - Najstabilniejsze wyniki (odchylenie Â±45.2)
  - 81% wskaÅºnik ukoÅ„czenia torÃ³w
  - Szybka konwergencja (210 epizodÃ³w treningu)

- **âš ï¸ Ograniczenia:**
  - Wymaga preprocessingu obrazu (84x84 grayscale)
  - Dyskretna przestrzeÅ„ akcji (5 akcji)
  - DÅ‚ugi czas treningu na poczÄ…tku

#### ğŸ­ **PPO (Proximal Policy Optimization)**

- **âœ… Mocne strony:**

  - Bardzo dobra Å›rednia nagroda (623.4 punktÃ³w)
  - CiÄ…gÅ‚a przestrzeÅ„ akcji (pÅ‚ynne sterowanie)
  - Stabilny trening dziÄ™ki Stable Baselines3
  - 63% wskaÅºnik ukoÅ„czenia

- **âš ï¸ Ograniczenia:**
  - WyÅ¼sza wariancja wynikÃ³w (Â±78.9)
  - Wymaga wiÄ™cej zasobÃ³w obliczeniowych
  - DÅ‚uÅ¼szy czas konwergencji

#### ğŸ§¬ **NEAT (NeuroEvolution)**

- **âœ… Mocne strony:**

  - Automatyczne projektowanie architektury sieci
  - Nie wymaga gradientÃ³w (brak backpropagation)
  - Ciekawe rozwiÄ…zania ewolucyjne
  - Dobra eksploracja przestrzeni rozwiÄ…zaÅ„

- **âš ï¸ Ograniczenia:**
  - NiÅ¼sza skutecznoÅ›Ä‡ (387.2 punktÃ³w)
  - Wysoka wariancja (Â±156.3)
  - DÅ‚ugi czas ewolucji (50+ generacji)
  - 32% wskaÅºnik ukoÅ„czenia

#### ğŸ² **Random Agent (Baseline)**

- **Cel:** Punkt odniesienia dla innych algorytmÃ³w
- **Wyniki:** Negatywne nagrody (-42.1), brak ukoÅ„czeÅ„
- **Wniosek:** Potwierdza trudnoÅ›Ä‡ Å›rodowiska CarRacing

## ğŸ”§ Architektura Techniczna

### ğŸ—ï¸ Struktura Projektu

```
car-racing-rl/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/                 # Implementacje agentÃ³w
â”‚   â”‚   â”œâ”€â”€ dqn_agent.py       # Double DQN + Experience Replay
â”‚   â”‚   â”œâ”€â”€ ppo_agent.py       # PPO (wÅ‚asna implementacja)
â”‚   â”‚   â”œâ”€â”€ neat_agent.py      # NEAT evolution
â”‚   â”‚   â””â”€â”€ random_agent.py    # Baseline
â”‚   â”œâ”€â”€ environments/          # Åšrodowiska i wrappery
â”‚   â”‚   â”œâ”€â”€ car_racing_env.py  # GÅ‚Ã³wne Å›rodowisko
â”‚   â”‚   â””â”€â”€ lap_completion_fix_wrapper.py  # Fix dla PPO
â”‚   â”œâ”€â”€ models/               # Architektury sieci
â”‚   â”‚   â””â”€â”€ neural_networks.py # CNN dla DQN/PPO
â”‚   â”œâ”€â”€ training/             # Skrypty treningu
â”‚   â”‚   â”œâ”€â”€ train_dqn.py     # Trening DQN
â”‚   â”‚   â”œâ”€â”€ train_ppo_test.py # Trening PPO (SB3)
â”‚   â”‚   â””â”€â”€ train_neat.py     # Ewolucja NEAT
â”‚   â””â”€â”€ evaluation/           # Testy i porÃ³wnania
â”‚       â”œâ”€â”€ simple_model_tester.py  # GÅ‚Ã³wne testy
â”‚       â””â”€â”€ record_best_models.py   # Nagrywanie najlepszych
â”œâ”€â”€ models/                   # Zapisane modele
â”‚   â”œâ”€â”€ dqn_model_ep210.keras
â”‚   â”œâ”€â”€ ppo_carracing1.zip
â”‚   â””â”€â”€ neat_best.pkl
â”œâ”€â”€ configs/                  # Konfiguracje
â”‚   â””â”€â”€ neat_config.txt       # Parametry NEAT
â””â”€â”€ results/                  # Wyniki i wykresy
    â””â”€â”€ comparison_plots.png
```

### ğŸ§  Architektury Sieci Neuronowych

#### **DQN Network:**

```python
Input: (84, 84, 1) grayscale
  â†“
Conv2D(32, 8Ã—8, stride=4) + ReLU
  â†“
Conv2D(64, 4Ã—4, stride=2) + ReLU
  â†“
Conv2D(64, 3Ã—3, stride=1) + ReLU
  â†“
Flatten() â†’ Dense(512) + ReLU
  â†“
Output: Dense(5) # [nic, lewo, prawo, gaz, hamuj]
```

#### **PPO Network:**

```python
Actor-Critic Architecture:
- Actor: CNN â†’ Dense(3)   # [steering, gas, brake]
- Critic: CNN â†’ Dense(1)  # Value function
```

#### **NEAT Network:**

```python
Evolutionary Architecture:
Input: 7056 neurons (84Ã—84 flattened)
  â†“
Hidden: Variable (evolved 0-50+ neurons)
  â†“
Output: 3 neurons [steering, gas, brake]
```

## ğŸš€ Instalacja i Uruchomienie

### Wymagania

```bash
Python 3.12
gymnasium[box2d]>=0.29.0
tensorflow>=2.0.0
stable-baselines3>=2.0.0
neat-python>=0.92
opencv-python>=4.5.0
numpy>=1.21.0
matplotlib>=3.5.0
pandas
scikit-learn
swig
seaborn>=0.12.0
moviepy>=1.0.3
```

### Instalacja

```bash
# Klonowanie repozytorium
git clone <repository-url>
cd car-racing-rl

# Instalacja zaleÅ¼noÅ›ci
pip install -r requirements.txt

# Instalacja Box2D (moÅ¼e wymagaÄ‡ dodatkowych narzÄ™dzi)
pip install box2d-py
```

### Uruchomienie TestÃ³w

```bash
# Test wszystkich agentÃ³w (30 epizodÃ³w kaÅ¼dy)
python src/evaluation/simple_model_tester.py

# Test tylko PPO
python src/training/evaluate_ppo.py

# Test konkretnego agenta
python src/evaluation/test_specific_agent.py --agent dqn --episodes 50
```

### Trening Nowych Modeli

```bash
# Trening DQN od zera
python src/training/train_dqn.py --episodes 200

# Kontynuacja treningu DQN
python src/training/continue_dqn_training.py --checkpoint checkpoints/dqn/dqn_model_ep100.keras

# Trening PPO (Stable Baselines3)
python src/training/train_ppo_test.py

# Ewolucja NEAT
python src/training/train_neat.py --generations 50
```

## ğŸ“Š Metodologia Eksperymentu

### Åšrodowisko Testowe

- **Gra:** CarRacing-v3 (Gymnasium)
- **Cel:** UkoÅ„czenie losowo generowanych torÃ³w wyÅ›cigowych
- **Nagroda:** +1000 za ukoÅ„czenie, -100 za wyjazd, -0.1 za kaÅ¼dy krok
- **Kryterium sukcesu:** Nagroda > 600 punktÃ³w

### ProtokÃ³Å‚ Testowania

- **Liczba epizodÃ³w:** 30 per agent (statystycznie znaczÄ…ce)
- **Maksymalny czas:** 1000 krokÃ³w per epizod
- **Tryb:** Deterministyczny (epsilon=0, deterministic=True)
- **Åšrodowisko:** Bez renderowania (szybsze testy)

### Metryki Oceny

1. **Åšrednia nagroda** Â± odchylenie standardowe
2. **WskaÅºnik ukoÅ„czenia** (% epizodÃ³w z nagrodÄ… >600)
3. **Najlepszy/najgorszy wynik** (zakres performance)
4. **StabilnoÅ›Ä‡** (konsystencja wynikÃ³w)

## ğŸ”¬ Wnioski Naukowe

### ğŸ¯ Kluczowe Odkrycia

1. **DQN dominuje w CarRacing**

   - Double DQN + Experience Replay = najlepsza kombinacja
   - Dyskretyzacja akcji nie szkodzi wydajnoÅ›ci
   - Stabilny trening dziÄ™ki target network

2. **PPO - solidny wybÃ³r uniwersalny**

   - Dobra wydajnoÅ›Ä‡ bez fine-tuningu
   - CiÄ…gÅ‚e akcje = bardziej naturalne sterowanie
   - Stable Baselines3 = production-ready

3. **NEAT - potencjaÅ‚ badawczy**

   - Automatyczne design sieci = fascynujÄ…ce
   - MoÅ¼e odkrywaÄ‡ nieintuicyjne rozwiÄ…zania
   - Wymaga wiÄ™cej pokoleÅ„ dla lepszych wynikÃ³w

4. **Znaczenie preprocessingu**
   - 84Ã—84 grayscale = optymalny kompromis
   - Normalizacja kluczowa dla stabilnoÅ›ci
   - Frame stacking moÅ¼e poprawiÄ‡ wyniki

### ğŸ” Implikacje Praktyczne

#### **Dla Praktyki RL:**

- **DQN** - wybÃ³r dla problemÃ³w z dyskretnÄ… przestrzeniÄ… akcji
- **PPO** - uniwersalny algorytm, dobry starting point
- **NEAT** - dla badaÅ„ nad architekturami sieci

#### **Dla CarRacing specyficznie:**

- Åšrodowisko reaguje dobrze na value-based methods (DQN)
- Preprocessing obrazu ma ogromne znaczenie
- 600+ punktÃ³w = prÃ³g kompetentnej jazdy

## ğŸ‘¥ TwÃ³rca

**Autor:** Jakub PakuÅ‚a
**Uniwersytet:** Uniwersytet GdaÅ„ski
**Przedniot:** Inteligencja Obliczeniowa  
**Semestr:** Informatyka Praktyczna4, Rok Akademicki 2024/25

### Techniczne Zasoby

- [Gymnasium Documentation](https://gymnasium.farama.org/)
- [Stable Baselines3](https://stable-baselines3.readthedocs.io/)
- [NEAT-Python](https://neat-python.readthedocs.io/)

## ğŸ® Quick Start

```bash
# Szybki test najlepszych modeli
python src/evaluation/simple_model_tester.py

# Wyniki pojawiÄ… siÄ™ w konsoli + wykres w results/
```

**ğŸ† Najlepszy Agent: DQN z 844.6 punktami Å›rednio!** ğŸš—ğŸ’¨

---

_Projekt wykonany w ramach kursu Inteligencja Obliczeniowa_  
_Ostatnia aktualizacja: Czerwiec 2025_
